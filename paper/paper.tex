\documentclass{sig-alternate}
\usepackage{algorithm,algpseudocode}
\usepackage{url}

\begin{document}

\conferenceinfo{HPDC}{'12 Delft, The Netherlands}
% \CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden
% - IF NEED BE. \crdata{0-12345-67-8/90/01}  % Allows default copyright data
% (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.

\title{Cost- and Deadline-constrained Scheduling of Scientific Workflow Ensembles in IaaS Clouds}
% \subtitle{[Extended Abstract]}

\numberofauthors{2}
\author{
    \alignauthor Maciej Malawski and Jarek Nabrzyski\\
       \affaddr{University of Notre Dame}\\
       \affaddr{Center for Research Computing}\\
       \affaddr{111 Information Technology Center}\\
       \affaddr{Notre Dame, IN, USA}\\
       \email{\{mmalawsk,naber\}@nd.edu}
    \alignauthor Gideon Juve and Ewa Deelman\\
       \affaddr{USC Information Sciences Institute}\\
       \affaddr{4676 Admiralty Way}\\
       \affaddr{Marina del Rey, CA, USA}\\
       \email{\{gideon,deelman\}@isi.edu}
}

\maketitle
 
\begin{abstract}
Abstract goes here.
\end{abstract}

%\category{H.4}{Information Systems Applications}{Miscellaneous}
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%\terms{Theory}

\keywords{Scientific workflows, cloud computing, DAG scheduling, simulation}

\section{Introduction}

Scientific workflows, usually represented as directed acyclic graphs (DAG), are
the important class of applications that have been studied in the context of
resource management and task scheduling on grid and utility computing systems.
However, large computational applications are often composed of several 
inter-related workflows grouped into {\em ensembles}. All the
workflows in an ensemble typically have a similar structure, but they differ by
input data, number of tasks and individual task sizes. A good example of
scientific workflow ensemble comes from the CyberShake
application~\cite{Callaghan11}, which calculates seismic hazards for a given
geographical region such as California. In order to produce a hazard map, the
hazard curves need to be computed for a set of geographical sites, while each
site requires running a single complex scientific workflow. Workflows for each
site may differ not only in their parameters, but also in their priority; e.g. some of
the points on the map correspond to urban areas or strategic locations such as
power plants, whereas others may be located in less populated regions. Another
example of workflow ensembles comes from astronomical applications, such as
Montage~\cite{Deelman08} or search for exo-solar planets from Kepler project
data~\cite{vockler11}, where multiple, different workflows are used to
process data for different areas of the sky.

Infrastructure-as-a-Service (IaaS) clouds offer the ability to provision 
computational resources on-demand according to a pay-per-use model. These systems 
are regarded by the scientific community as a potentially attractive source of 
low-cost, on-demand computing resources~\cite{Ostermann10,Keahey09}. In contrast 
to clusters and grids, which typically offer best-effort quality of service, clouds 
give users more flexibility to creating a controlled and managed computing environment.
Clouds provide the ability to adjust resource capacity according to the dynamically changing
computing demands of the application, often called auto-scaling. However, giving users 
more control over the resources also requires the development of new methods for task 
scheduling and resource provisioning. The resource management decisions required 
in cloud scenarios not only have to take into account traditional performance-related 
metrics such as the workflow makespan or the resource utilization, but must also now consider
budget constraints, since the resources from public (commercial) cloud providers 
usually have monetary costs associated with them~\cite{Durkee10}.

In this paper, we aim to gain insight into resource management challenges when executing 
scientific workflow ensembles on clouds. In particular we address a new important problem of maximizing
the number of completed workflows from an ensemble under both budget and
deadline constraints. The motivation is to answer the fundamental question of a
researcher: How much computation can be completed given the limited budget and 
timeframe of a research project? The goals of this paper are to discuss and assess possible
static and dynamic strategies for both task scheduling and resource
provisioning. Based on the knowledge of workflow scheduling algorithms we
analyze strategies for on-line scheduling of individual tasks as well as static
algorithms that rely on the information about the workflow structure (critical
paths and workflow levels) and estimates of task runtimes. In addition, we
analyze a hybrid workflow-aware dynamic scheduling algorithm, which uses the
workflow structure information to estimate which workflows should be rejected
from the ensemble due to the constraints. We evaluate the algorithms via simulation. 
We have developed cloud workflow simulator based on
CloudSim~\cite{Calheiros11}, which models the infrastructure and the application. The
algorithms are subject to evaluation on a set of scientific workflow ensembles with a broad range of
budget and deadline parameters. 

The paper is organized as follows: after analysis of related work in
Section~\ref{sec:related}, Section~\ref{sec:problem} gives the details about the
infrastructure and application model we are targeting in this paper.
In Section~\ref{sec:algorithms} we describe the dynamic and
static algorithms we developed. Section~\ref{sec:performance} presents test
scenarios, metrics and results of simulation studies of these algorithms.
Finally, general conclusions, lessons learned and future work are outlined in
Section~\ref{sec:conclusions}.

\section{Related Work}
\label{sec:related}
General policy and rule-based approaches to dynamic provisioning (e.g. Amazon
Auto Scaling\footnote{\url{http://aws.amazon.com/autoscaling}} and
RightScale\footnote{\url{http://www.rightscale.com}}) allow adjusting the size
of resource pool based on metrics related to infrastructure and application.
A typical infrastructure-specific metric is system load, whereas
application-specific metrics include response time and length of task or
request queue. It is possible to set thresholds and limits to tune the behavior
of these autoscaling systems, but no support for complex applications is provided.

Policy-based approaches for scientific workloads (e.g. \cite{Marshall2010,
Kim2011}) also allow to scale the the cloud resource pool or to extend the
capabilities of clusters using cloud-burst techniqies. Our approach is different
in that we consider workflows, while policy based approaches typically consider
bags of independent tasks or unpredictable batch workloads. This enables us to
take advantage of workflow-aware scheduling heuristics that cannot be applied to
independent tasks.


Our work is related to the strategies for deadline-constrained cost-minimization
workflow scheduling, developed for utility grid systems. However, our problem is
different from \cite{Yu2005, Abrishami2010} in that we consider ensembles of workflows in
IaaS clouds, which allow one to provision resources on a per-hour billing model,
rather than utility grids, which allow one to choose from a pool of existing
resources with a per-job billing model. Our work is also different from
cloud-targeted autoscaling solution~\cite{Mao2011} in that we consider ensembles
of workflows rather than unpredictable workloads containing workflows. We also have budget constraints
rather than cost minimization as a goal. In other words, we assume that there is
more work to be done than the available budget, so some work must be rejected.
Therefore cost is not something we optimize, but rather a constraint.


This work is related to bi-criteria scheduling and multi-criteria scheduling of
workflows~\cite{Wieczorek2009,Prodan10,Dongarra2007}. These approaches are
similar to ours in that we have two scheduling criteria: cost and makespan. The
challenge in multi-criteria scheduling is to derive an objective function that
takes into account all of the criteria. In our case one objective (amount
of work completed) is subject to optimization, whereas time and cost are
treated as constraints. Other approaches~\cite{Talukder2009,Pandey2010} use
metaheuristics that usually run for a long time before producing good results,
which makes them less useful in practice. Our work can be also
regarded as an extension of the budget-constrained workflow scheduling
\cite{Sakellariou2007} in the sense that we are dealing with workflow ensembles
and the deadline constraint is added.
 
\section{Problem Description}
\label{sec:problem}

\subsection{Resource Model}

We assume a resource model similar to Amazon's Elastic Compute Cloud (EC2). In
this model, virtual machine (VM) instances are requested on-demand. The VMs are
billed by the hour, with partial hours being rounded up. Although there are
multiple VM types with different amounts of CPU, memory, disk space, and I/O,
for this paper we focus on a single VM type because we assume that for most
applications there will typically be one VM type with the best price/performance
ratio for the application~\cite{Juve2009}. We also assume that a submitted task
has exclusive access to a VM instance and that there is no preemption.


\subsection{Application Model}
The target applications for this paper are scientific workflows that can be
modeled as Directed Acyclic Graphs (DAGs), where the nodes in the graph
represent computational tasks, and the edges represent data- or control-flow
dependencies between the tasks.


We assume that we have runtime estimates for each task in the workflow based on
either a performance model of the application, or historical data that can be
mined. In particular, in this study we use workflows from the synthetic workflow
generator~\cite{Bharathi08}.


Although scientific workflows are often data-intensive, we do not currently
consider the size of input and output data when scheduling tasks. Instead we
assume that all workflow data is stored in a shared cloud storage system, such
as Amazon S3, and that data transfer times are included in task runtime
estimates. We also assume that data transfer times between the shared storage
and the VMs are equal for different VMs so that task placement decisions do not
have an impact on the runtime of the tasks.


This paper focuses on scheduling and provisioning for workflow ensembles. A
workflow ensemble consists of several related workflows that a user wishes to
run. Each workflow in an ensemble is given a priority that indicates how
important the workflow is to the user.


The goal of this workflow ensemble scheduling and cloud provisioning problem is
to complete as many high-priority workflows as possible given a fixed budget and
deadline.


\section{Algorithms}
\label{sec:algorithms}

In this section we describe several algorithms we developed to execute ensembles of
workflows on the cloud under budget and deadline constraints.

\subsection{Static Provisioning Dynamic Scheduling \\
(SPDS) }
\label{sec:spds}

The simplest strategy for executing ensembles of workflows in the cloud is to
provision resources statically and schedule workflow tasks dynamically. Given a 
budget $b$, deadline $d$, and the hourly price $p$ of a VM, it is easy to calculate
the number of VMs, $N_{VM}$, to provision so that the entire budget is consumed before
the deadline:

\begin{equation}
\label{eq:static-plan}
N_{VM} = \lceil b / d / p \rceil
\end{equation}

The SPDS algorithm statically provisions $N_{VM}$ VMs at the start of the ensemble execution 
and keeps them running until the deadline is reached, or the budget runs out.
This provisioning plan has the advantage that it minimizes the number of provisioning 
and de-provisioning requests.

Once the VMs are provisioned, the tasks are mapped onto idle VMs using the dynamic
priority-based scheduling procedure shown in Algorithm~\ref{alg:ds}.

\begin{algorithm}
\caption{Priority-based scheduling algorithm for SPDS}
\label{alg:ds}
\begin{algorithmic}[1]
\Procedure{Schedule}{}
    \State $P\gets$ empty priority queue
	\State $IdleVMs\gets$ set of idle VMs
	\For{root task $t$ in all workflows}
    	\State \Call{Insert}{$t,P$} 
    \EndFor
    \While{deadline not reached}
    	\While{$IdleVMs \neq \emptyset\ and\ P \neq \emptyset$}
    		\State $v\gets$ \Call{SelectRandom}{$IdleVMs$}
    		\State $t\gets$ \Call{Pop}{$P$}
    		\State \Call{Submit}{$t,v$}
    	\EndWhile
    	\State Wait for task $t$ to finish on VM $v$
    	\State Update $P$ with ready children of $t$
		\State \Call{Insert}{$v,IdleVMs$}
    \EndWhile
\EndProcedure
\end{algorithmic} 
\end{algorithm}

Initially, the ready tasks from all workflows in the ensemble are added to a 
priority queue based on the priority of the workflow to which they belong. If
there are idle VMs available, and the priority queue is not empty, the next task
from the priority queue is submitted to a randomly chosen idle VM. The process is 
repeated until there are no idle VMs or the priority queue is empty. The scheduler
then waits for a task to finish, adds its ready children to the priority queue,
marks the VM as idle, and the entire process repeats until the deadline is reached.

This algorithm guarantees that tasks from lower priority workflows are
always deferred when higher-priority tasks are available, but lower-priority
tasks can still occupy idle VMs when higher-priority tasks are not available. 
However, because there is no preemption, long-running low-priority tasks may delay 
the execution of higher-priority tasks. In addition, tasks from low priority 
workflows may be executed even though there is no chance that those workflows 
will be completed within the current budget and deadline. Fig.~\ref{fig:spds-example} 
shows an example schedule generated using the SPDS algorithm. The figure illustrates 
how tasks from lower priority workflows backfill idle VMs when tasks from higher 
priority workflows are not available.

\begin{figure}[htb] 
\centering
\includegraphics[width=1.0\columnwidth]{figures/spds-gantt}
 \caption{Example schedule generated using SPDS algorithm. The colors indicate how various tasks from different workflows are scheduled onto Virtual Machines. }
\label{fig:spds-example}
\end{figure}

\subsection{Dynamic Provisioning Dynamic \\Scheduling (DPDS)}
 
The static provisioning approach of SPDS may not be optimal when resource demand
changes during ensemble execution. In those cases, SPDS may either over-provision, 
wasting the budget on idle VMs, or under-provision, using up the deadline while starving 
the application of usable VMs. One of the advantages of elasticity in cloud computing 
is that the number of resources can be changed to meet the demands of an application.
In policy-based provisioning (or auto-scaling) systems, metrics such as resource 
utilization or task queue length are monitored to estimate application demand.
The DPDS algorithm uses this approach to determine when to provision and de-provision VMs. The 
algorithm periodically computes resource utilization as the percentage of idle VMs, and
adjusts the number of VMs if the utilization is above or below given thresholds.
Because it is assumed that VMs are billed by the hour, DPDS only considers VMs that are
approaching their hourly billing cycle when deciding which VMs to terminate. This process
is described in Algorithm~\ref{alg:prov}.

\begin{algorithm}
\caption{Dynamic provisioning algorithm for DPDS}
\label{alg:prov}
\begin{algorithmic}[1]
\Require $c$: consumed budget; $b$: total budget; $d$: deadline; $p$: price;
$t$: current time; $u_h$: upper utilization threshold; $u_l$: lower utilization
threshold; $v_{max}$: maximum number of VMs
\Procedure{Provision}{}
	\State $V_R\gets$ set of running VMs
    \State $V_C\gets$ set of VMs completing billing cycle
    \If{$ b - c < |V_C| * p\ or\ t > d $ }
    	\State $n_T\gets |V_R| - \lfloor(b-c)/p\rfloor$
    	\State $V_T\gets$ select $n_T$ VMs to terminate from $V_C$
    	\State \Call{Terminate}{$V_T$} \label{l:terminate1}
    \Else 
		\State $u\gets$ current VM utilization
    	\If{$u>u_h$ and $|V_R| < v_{max}*N_{VM}$}
    		\State \Call{Start}{$new\ VM$}
    	\ElsIf{$u<u_l$}
    		\State $V_I\gets$ set of idle VMs
    		\State $n_T\gets \lceil|V_I|/2\rceil$ \label{l:nT2}
			\State $V_T\gets$ select $n_T$ VMs to terminate from $V_I$
    		\State \Call{Terminate}{$V_T$} \label{l:terminate2}
    	\EndIf 
    \EndIf
\EndProcedure
\end{algorithmic} 
\end{algorithm}

The set of VMs completing their billing cycle is determined by considering both the
provisioner interval, and the termination delay of the provider. This guarantees 
that VMs can be terminated before they start the next billing cycle and prevents the
budget from being overrun. The VMs terminated in line \ref{l:terminate1} of Algorithm~\ref{alg:prov} are the ones 
that would overrun the budget if not terminated in the current provisioning cycle. 
The VMs terminated on line \ref{l:nT2} are chosen to increase the resource utilization
to the desired threshold. In order to prevent instances from being
terminated too quickly, potentially wasting resources that have already been paid for
but could be used later, no more than half of the idle resources are terminated during
each provisioning cycle. To avoid an uncontrolled increase in the number of
instances, which may happen in the case of highly parallel workflows, the provisioner 
will not start a new VM if the number of running VMs is greater than the product of
$N_{VM}$ and an autoscaling factor, $v_{max}$ that has to be set manually.


\subsection{Workflow-Aware DPDS (WA-DPDS)}

The DPDS algorithm does not use any information about the structure of the workflows
in the ensemble when scheduling tasks: it looks only at the priorities of the ready 
tasks when deciding which task to schedule next. It does not consider whether a lower 
priority task belongs to a workflow that will never be able to complete given the 
current budget and deadline. As a result, DPDS may execute lower priority tasks just 
to keep VMs busy and that will end up delaying higher priority tasks, making it less likely
that higher priority workflows will be able to finish.

In order to address this issue, the Workflow-Aware DPDS (WA-DPDS) algorithm extends 
DPDS by introducing a workflow admission procedure. The admission procedure is
invoked whenever WA-DPDS sees the first task of a new workflow at the head of the 
priority queue (i.e. when no other tasks from the workflow have been scheduled yet). 
The admission procedure---shown in Algorithm~\ref{alg:wa-dpds}---estimates whether there is 
enough budget remaining to admit the new workflow; if there is not, then the 
workflow is rejected and its tasks are removed from the queue. This algorithm 
compares the current cost (consumed budget) and remaining budget, taking
into account the cost of currently running VMs, and the cost of workflows 
that have already been admitted. It adds a small safety margin to  avoid going over
the budget.

\begin{algorithm}
\caption{Workflow admission algorithm for WA-DPDS}
\label{alg:wa-dpds}
\begin{algorithmic}[1]
\Require $w$: workflow; $b$: budget; $c$: current cost
\Procedure{Admit}{$w,b,c$}
    \State $r_n\gets b-c$ \Comment{Budget remaining for new VMs}
    \State $r_c\gets $cost committed to VMs that are running
    \State $r_a\gets $cost to complete workflows previously admitted
	\State $r_m\gets 0.1$ \Comment{Safety margin}
	\State $r_b\gets r_n+r_c-r_a-r_m$ \Comment{Budget remaining}
	\State $c_w\gets$ \Call{EstimateCost}{w}
    \If{$c_w<r_b$}
    	\State \textbf{return} $TRUE$
    \Else
    	\State \textbf{return} $FALSE$
	\EndIf    	 
\EndProcedure
\end{algorithmic} 
\end{algorithm}


This admission procedure relies only on the total estimated resource consumption and
compares it to the remaining budget. We found that this estimation is useful not
only to prevent low-priority workflows from delaying high-priority ones, but
also to reject large and costly workflows that would overrun the budget and
admit smaller workflows that can efficiently utilize idle resources in ensembles 
containing workflows of non-uniform sizes. It would also be possible to extend this 
admission procedure to check other constraints, such as whether the estimated 
critical path of the new workflow exceeds the time remaining until the deadline.



\subsection{Static Provisioning Static Scheduling \\(SPSS)}

The previous algorithms are all online algorithms that make provisioning 
and scheduling decisions at runtime. In comparison, the SPSS algorithm 
creates a provisioning and scheduling plan before running any workflow tasks.
This enables SPSS to start only those workflows that it knows can be completed
given the deadline and budget constraints, and eliminates any waste that may
be allowed by the dynamic algorithms.

The approach used by SPSS is to plan each workflow in the ensemble in priority 
order, rejecting any workflows that cannot be completed by the deadline or that 
cause the cost of the plan to exceed the budget. Once the plan is complete, then 
the VMs are provisioned and tasks are executed according to the schedules given
by the plan. However, in this paper we only consider a deterministic environment and task times.

The disadvantage of the static planning approach used by SPSS is that it is 
sensitive to dynamic changes in the environment and the application that may 
disrupt the carefully constructed plan.

Algorithm~\ref{alg:admit} shows how ensembles are planned in SPSS. The algorithm
starts with an empty plan containing no VMs and no scheduled tasks. Workflows
from the ensemble are considered in priority order. For each workflow, SPSS
attempts to build on top of the current plan by provisioning VMs to schedule 
the tasks of the workflow so that it finishes before the deadline with the least
possible cost. If the cost of the new plan is less than the budget, then the 
new plan is accepted and the workflow is admitted. If not, then the new plan is 
rejected and the process continues with the next workflow in the ensemble. The 
idea behind this algorithm is that, if each workflow can be completed by the 
deadline with the lowest possible cost, then the number of workflows that can 
be completed within the given budget will be maximized.

\begin{algorithm}
\caption{Ensemble planning algorithm for SPSS}
\label{alg:admit}
\begin{algorithmic}[1]
\Require $W$: workflow ensemble; $b$: budget; $d$: deadline
\Ensure Schedule as much of $W$ as possible given $b$ and $d$
\Procedure{PlanEnsemble}{$W,b,d$}
    \State $P\gets \emptyset$ \Comment{Current plan}
    \State $A\gets \emptyset$ \Comment{Set of admitted DAGs}
    \For{$w\ in\ W$}
        \State $P^\prime \gets$\ \Call{PlanWorkflow}{$w,P,d$}
        \If{$Cost(P^\prime) \le b$}
            \State $P\gets\ P^\prime$ \Comment{Accept new plan}
            \State $A \gets A\ +\ w$ \Comment{Admit w}
        \EndIf
    \EndFor
    \State \textbf{return} $P,A$
\EndProcedure
\end{algorithmic}
\end{algorithm}

In order to plan a workflow, the SPSS algorithm assigns 
sub-deadlines to each individual task in the workflow, and then schedules 
each task so as to minimize the cost of the task while still meeting its 
assigned sub-deadline. The idea is that if each task can be completed by 
its deadline in the least expensive manner possible, then the cost of the
entire workflow can be minimized without exceeding the ensemble deadline.

SPSS assigns sub-deadlines to each task based on the slack time of the
workflow, which is defined as the amount of extra time that a workflow
can extend its critical path and still be completed by the ensemble deadline.
For a workflow $w$, the slack time of $w$ is:

$$
ST(w) = d - CP(w)
$$

where $d$ is the ensemble deadline and $CP(w)$ is the critical path of $w$.
We assume that $CP(w) \leq d$, otherwise the workflow cannot be completed 
by the deadline and must be rejected. For large ensembles we expect the 
critical path of any individual workflow to be much less than the deadline.

A task's level is the length of the longest path between the task and an entry
task of the workflow:

$$
Level(t) = \begin{cases}
0,&$if~$Pred(t) = \emptyset\cr
\max_{p \in Pred(t)}Level(p)+1,&$otherwise.$
\end{cases}
$$

SPSS distributes the slack time of the workflow by level, so that each level
of the workflow gets a portion of the workflow's slack time proportional to
the number of tasks in the level and the total runtime of tasks in the level.
The idea is that levels containing many tasks and large runtimes should be given
a larger portion of the slack time so that tasks in those levels may be serialized.
Otherwise, many resources need to be allocated to run all of the tasks in 
parallel, which may be more costly.

The slack time of a level $l$ in workflow $w$ is given by:

$$
ST(l) = ST(w) \Biggl[\left({\alpha}\frac{N(l)}{N(w)}\right) + \left({(1 - \alpha)}\frac{R(l)}{R(w)} \right)\Biggr]
$$

where $N(w)$ is the number of tasks in the workflow, $N(l)$ is the number of tasks 
in level $l$, $R(w)$ is the total runtime of all tasks in the workflow, $R(w)$ is
the total runtime of all tasks in level $l$, and $\alpha$ is a parameter between
0 and 1 that causes more slack time to be given to levels with more tasks (large 
$\alpha$) or more runtime (small $\alpha$).

The deadline of a task $t$ is then:

$$
DL(t) = LST(t) + RT(t) + ST(Level(t))
$$

where $Level(t)$ is the level of $t$, $RT(t)$ is the runtime of $t$, and $LST(t)$ 
is the latest start time of $t$ determined by:

$$
LST(t) = \begin{cases}
0,&$if~$Pred(t) = \emptyset\cr
\max_{p \in Pred(t)} DL(p),&$otherwise.$
\end{cases}
$$


Algorithm~\ref{alg:planworkflow} shows how SPSS creates low-cost plans for
each workflow. The \textsc{PlanWorkflow} procedure first calls 
\textsc{DeadlineDistribution} to assign sub-deadlines to tasks according
to the procedure described above. Then, the \textsc{PlanWorkflow} procedure
schedules tasks onto VMs, allocating new VMs when necessary. For each task 
in the workflow, the the least expensive slot is chosen to schedule the 
task so that it can be completed by  its deadline. VMs are allocated in 
blocks of one billing cycle (one hour) regardless of the size of the task. 
When computing the cost of scheduling a task on a given VM, the algorithm 
considers idle slots in blocks that were allocated for previous tasks to be 
free, while slots in new blocks cost the full price of a billing cycle. For 
example, if a task has a runtime of 10 minutes, and the price of a block is 
\$1, then the algorithm will either schedule the task on 
an existing VM that has an idle slot larger than 10 minutes for a cost of \$0, 
or it will allocate a new block on an existing VM, or provision a new VM, for 
a cost of \$1. If the cost of slots on two different VMs is equal, then the 
slot with the earliest start time is chosen. To prevent too many VMs from 
being provisioned, the algorithm always prefers to extend the runtime of 
existing VMs before allocating new VMs. The result of this is that the 
algorithm will only allocate a new block if there are no idle slots on 
existing blocks large enough or early enough to complete the 
task by its deadline, and it will only allocate a new VM if it cannot add 
a block to an existing VM to complete the task by its deadline.

\begin{algorithm}
\caption{Workflow planning algorithm for SPSS}
\label{alg:planworkflow}
\begin{algorithmic}[1]
\Require $w$: workflow; $P$: current plan; $d$: deadline
\Ensure Create plan for $w$ that minimizes cost and meets deadline $d$
\Procedure{PlanWorkflow}{$w,P,d$}
    \State $P^\prime\gets$ copy of $P$
    \State \Call{DeadlineDistribution}{w,d}
    \For{$t\ in\ w\ sorted\ by\ DL(t)$}
        \State $v \gets$ VM that minimizes cost and start time of t
        \If{$FinishTime(t,v) < DL(t)$}
            \State Schedule(t,v)
        \Else
            \State Provision a new VM v
            \State Schedule(t,v)
        \EndIf
    \EndFor
    \State \textbf{return} $P^\prime$
\EndProcedure
\end{algorithmic}
\end{algorithm}

An example schedule generated by SPSS is shown in Fig.~\ref{fig:spss-example}.
This example shows how SPSS tends to start many workflows in parallel, running
each workflow over a longer period of time on only a few VMs to minimize cost. 
In comparison, the dynamic algorithms tend to run one workflow at a time across 
many VMs in parallel.

\begin{figure}[htb] 
\centering
\includegraphics[width=1.0\columnwidth]{figures/spss-gantt}
 \caption{Example of schedule generated using SPSS algorithm. Tasks are colored by workflow. }
\label{fig:spss-example}
\end{figure}



\section{Performance Evaluation}
\label{sec:performance}



\subsection{Simulator}

To evaluate and compare our algorithms, we developed a cloud workflow simulator based on
CloudSim~\cite{Calheiros11}. Our simulation model consists of Cloud, VM and WorkflowEngine 
entities. The Cloud entity is responsible for staring and terminating VM entities using 
an API similar to Amazon EC2. VM entities simulate the execution of individual tasks,
and the WorkflowEngine entity manages the scheduling of tasks and the provisioning of
VMs. We assume that the VMs have a single core and execute tasks sequentially. Although 
CloudSim provides a more advanced infrastructure model which includes time-sharing and 
space-sharing policies, and power consumption in a datacenter, we do not use these 
features, since we are interested mainly in the execution of tasks on VMs and high-level 
workflow scheduling and provisioning. The simulator reads workflows in a modified version of
the DAX format used by the Pegasus Workflow Management System~\cite{Deelman2005}. The modified format was
created for the synthetic workflow generator developed by Bharathi, et al~\cite{Bharathi08}.



\subsection{Scenarios}
\label{sec:scenarios}

We have prepared test scenarios to evaluate the proposed algorithms under a
range of parameters that enables us to observe the interesting characteristics
of the different algorithms for smaller and larger constraints. For a given
ensemble we can expect, in general, that the amount of work done (in terms of
workflows completed) depends mainly on the budget constraint, which determines
the number of machine-hours that can be provisioned. The deadline, although
important in terms of scheduling, is assumed to be greater than the critical
path of every workflow. We can then have a trivial but unrealistic solution
where the deadline  is long enough to allow all the workflow tasks to be
executed sequentially: this eliminates all the overheads of parallel execution
and leads to the highest resource utilization. More interesting and more
realistic, however, are the cases where the deadline is shorter than the serial
schedule, since it forces the system to provision multiple VMs to execute
workflow tasks in parallel.


\begin{figure}[tb] 
\centering
\includegraphics[width=0.6\columnwidth]{figures/ensemble-pareto}
\caption{Pareto-like distribution of workflow sizes in ensemble. Size is
measured in number of tasks.}
\label{fig:ensemble-distribution}
\end{figure}

The number of workflows in an ensemble depends on the particular application, but
we assume that ensemble sizes are on the order of between 10 and 100 workflows.
This range is motivated by several reasons: First, such sizes are typical of the
real applications we have examined. For example, the number of geographical 
sites of interest to the users of the CyberShake application is on the order 
of 100. Second, smaller ensembles consisting of just a few workflows can be 
aggregated into a single workflow, so there is no need to treat them as an
ensemble. Similarly, when the number of workflows is larger than 100, and each 
workflow has a large number of tasks, either the constraints are low enough to
prevent many of the workflows from running, or the problem of efficiently 
allocating them to the resources becomes similar to the bag-of-tasks problem, 
which is easier to solve efficiently.

\begin{figure*}[t] 
\centering
\includegraphics[width=0.19\textwidth]{figures/pareto-MONTAGE-n-1000-8-dagh1-20m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-CYBERSHAKE-n-1000-8-dagh1-20m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-LIGO-n-1000-8-dagh1-40m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-GENOME-n-1000-8-dagh100-1500m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-SIPHT-n-1000-8-dagh5-50m0.pdf}
\caption{Number of workflows completed for DPDS, AW-DPDS and SPSS
algorithms on ensembles of 100 Pareto-distributed workflows, {\em small budget}
(top) and {\em large budget} (bottom).}
\label{fig:number-complete-pareto}
\end{figure*}

%\begin{figure}[b!] 
%\centering
%\includegraphics[width=0.19\textwidth]{figures/pareto-score-MONTAGE-n-1000-8-dagh1-20m0.pdf}
%\includegraphics[width=0.19\textwidth]{figures/pareto-score-CYBERSHAKE-n-1000-8-dagh1-20m0.pdf}
%\includegraphics[width=0.19\textwidth]{figures/pareto-score-LIGO-n-1000-8-dagh1-40m0.pdf}
%\includegraphics[width=0.19\textwidth]{figures/pareto-score-GENOME-n-1000-8-dagh100-1500m0.pdf}
%\includegraphics[width=0.19\textwidth]{figures/pareto-score-SIPHT-n-1000-8-dagh5-50m0.pdf}
%\caption{Comparison of algorithms based on score, which  is computed as
%$\sum(1/(p+1))$ where priorities of subsequent workflows are $p=0,1,2,\ldots$
%(0 means highest priority).}
%\label{fig:score}
%\end{figure}

In order to evaluate the algorithms on a standard set of workflows, we used
ensembles constructed from the workflows available\footnote{\url{https://confluence.pegasus.isi.edu/display/pegasus/WorkflowGenerator}}
in the workflow generator gallery~\cite{Bharathi08}. The gallery contains synthetic
workflows modeled on real applications. We used workflows from five applications 
that were managed by the Pegasus Workflow Management System. For each
application we constructed ensembles of 100 workflows by sampling collections of
workflows of various sizes (number of tasks) based on a given distribution.
Two distributions are considered:

\begin{itemize}
  \item constant distribution
  \item Pareto-like distribution.
\end{itemize}

The constant distribution produces ensembles containing of workflows with the same number of tasks per workflow, 
while the Pareto-like distribution produces ensembles with a small number of larger 
workflows and a large number of smaller workflows, as seen in 
Fig.~\ref{fig:ensemble-distribution}. The number of large workflows (of size $\geq$ 900) 
is slightly increased to represent the ``heavy-tail'', i.e. that there is a 
not-insignificant number of large workflows in the ensemble. We assume that large
workflows are more important to users, so in the Pareto ensembles the workflows are 
assigned priorities according to their size so that the largest workflows are given 
the highest priority. In future work we plan to explore other ensemble types such as
uniform distributions.

\begin{figure*}[t]  
\centering
\includegraphics[width=0.19\textwidth]{figures/pareto-size-MONTAGE-n-1000-8-dagh1-20m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-size-CYBERSHAKE-n-1000-8-dagh1-20m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-size-LIGO-n-1000-8-dagh1-40m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-size-GENOME-n-1000-8-dagh100-1500m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-size-SIPHT-n-1000-8-dagh5-50m0.pdf}
\caption{ Total work completed expressed as the sum of runtimes (in hours) of
tasks of completed workflows. Data computed for DPDS, AW-DPDS and SPSS
algorithms on ensembles of 100 Pareto-distributed workflows}
\label{fig:total-time}
\end{figure*}

In the experiments we assumed that all the VMs are homogeneous, have a
processing speed of 1000 MIPS (million instructions per second), and a price of
\$1 per VM-hour. We also assume that the sizes of tasks in the workflow gallery
are given in seconds measured on a 1000 MIPS machine. These values were chosen
to approximate realistic numbers, and also to simplify interpretation of
results. Moreover, the absolute values of these parameters do not affect the
relative comparison between the algorithms. In this study we do not take into
account the heterogeneity of the infrastructure since we assume that it is
always possible to select a VM type that has the best price to performance ratio
for a given application.


For each application, we selected ranges of constraints (deadline and budget) so
that it is possible to observe the possibly broad result space: from tight 
constraints, where only a small number of workflows can be completed, to 
more liberal constraints where all, or almost all, of the workflows can be 
completed. These parameters vary among the applications. Montage and CyberShake 
ensembles required budgets in the \$20 -- \$150 range and deadlines from 1 to
20 hours to be completed. The LIGO and SIPHT workflows have more long-running
tasks, so their ensembles required budgets up to \$1,500 and deadlines up to 
40 and 60 hours. The Epigenomics workflow is even more costly, requiring 
approximately \$12,000 and 1500 hours to complete a full ensemble. These
differences in total runtime and cost come from the fact that we used the
same workflow size distribution for all five applications, but
the sizes of tasks vary between applications.

We show the results for three algorithms: DPDS, WA-DPDS and SPSS. These 
experiments were run with maximum autoscaling factor set to 1.0 for
DPDS and WA-DPDS. After experimenting with DPDS and WA-DPDS we found that, 
due to the high parallelism of workflows, the resource utilization remains 
high most of the time, which leads to the rapid provisioning of new resources 
at the beginning of ensemble execution. This, in turn, results in less 
efficient total utilization due to high levels of parallelism. Setting the 
scaling limit to 1.0, ensures that the shape of the resource provisioning 
plan  is close to a rectangle (see Fig.~\ref{fig:spds-example}) and the VM 
termination procedure (Algorithm~\ref{alg:prov}) allows maximum utilization 
of VMs before shutdown. Based on experiments with different applications, 
we set the $\alpha$ parameter for deadline distribution in SPSS to be 0.7.



\subsection{Metrics}

There are many different ways to evaluate and compare the performance of 
the algorithms. The set of ensemble performance metrics we have used to
compare our algorithms includes:

\begin{itemize}
  \item $N_c$: number of workflows completed,
  \item $s$: user preference score (utility function),
  \item $T_C$: total amount of computing time completed,
  \item $C_{eff}$: effective cost of computing per hour, which includes
  overheads due to cost of idle resources.
\end{itemize}

The number of workflows completed is a simple metric that can be used for
comparison. However, because our ensembles contain workflows with different 
priorities and sizes, a less efficient algorithm may schedule smaller
workflows first, completing a large number of low-priority workflows. It may 
also happen that a conservative admission algorithm may overestimate the cost 
of a large workflow, thus rejecting high-priority workflows which actually 
could fit into the schedule. Therefore, we defined the user preference score as:

\begin{equation}
\label{eq:score}
s = \sum_{workflows\ completed}{2^{-p}}
\end{equation}

where $p$ is the priority of the workflow such that the highest-priority workflow 
has $p=0$, the next highest workflow has $p=1$, and so on. This exponential scoring 
function gives the highest priority workflow a score that is higher than all the 
lower-priority workflows combined:

\begin{equation}
\label{eq:score-property}
2^{-p} > \sum_{i=p+1,\ldots}2^{-i}
\end{equation}

This scoring is consistent with our definition of the problem, which is to 
complete as many workflows as possible, according to their priorities, given 
a budget and deadline.

The total amount of computing time completed, $T_C$, is calculated as the sum 
of the runtimes of all tasks in all completed workflows. Because we assumed 
that the resource cost per hour is equal to \$1, the total computing time 
can be easily compared to the allocated budget $b$. Their ratio: 
$u = (T_C * \$1)/b$, is the resource utilization, and its inverse: 
$C_{eff} = b/T_C$, is the effective resource cost in dollars per hour. To
calculate $C_{eff}$ and $u$ we use the value of allocated budget $b$, not the
actual total cost, since we are interested in the scenarios when the budget is
limited and does not allow completing all the workflows from the ensemble.



\subsection{Discussion of Results}

We ran simulations for constraint values ranging between the estimated 
limits indicated in Section~\ref{sec:scenarios}, resulting in approximately 
50-150 (budget, deadline) data points for each application. The runs were
performed for Pareto-distributed and constant workflow ensembles. From this result
space we selected representative plots. 

\begin{figure*}[htb] 
\centering
\includegraphics[width=0.19\textwidth]{figures/pareto-cost-MONTAGE-n-1000-8-dagh1-20m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-cost-CYBERSHAKE-n-1000-8-dagh1-20m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-cost-LIGO-n-1000-8-dagh1-40m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-cost-GENOME-n-1000-8-dagh100-1500m0.pdf}
\includegraphics[width=0.19\textwidth]{figures/pareto-cost-SIPHT-n-1000-8-dagh5-50m0.pdf}
\caption{Effective computing cost in \$ per hour calculated by dividing the sum of
task runtimes by the total budget. Higher costs result from lower resource utilization. Data based on DPDS, AW-DPDS and SPSS algorithms for ensembles of 100
Pareto-distributed workflows.}
\label{fig:cost}
\end{figure*}

Fig.~\ref{fig:number-complete-pareto} shows the number of the workflows
completed for two budget values for each application ensemble. The budget values
selected correspond to two extreme cases: {\em small budget} (top) and {\em
large budget} (bottom). The small budgets permit the algorithms to complete 
only a small number of workflows from the ensemble, while the larger budgets 
enable the algorithms to finish almost all the workflows. In three of five 
cases the SPSS algorithm performs better for tighter constraints. When the 
budget is low, or the deadline is short, the static procedure is able to more 
efficiently pack workflow tasks onto limited resources. This is the case for 
the LIGO, Epigenomics and SIPHT ensembles for small budgets, and for large
budgets with short deadlines. The exceptions are the Montage and CyberShake 
ensembles,  which consist of many fine-grained tasks. In these applications, 
as well for the other ensembles with large budgets and deadlines, the simple 
dynamic scheduling techniques perform equally well or better. The reason for 
this behavior is that the large number of tasks guarantees that resources 
are almost never idle regardless of how the tasks are scheduled. We also 
observe that WA-DPDS performs better than the workflow-unaware version, DPDS. 
This means that for the workflows we tested the simple admission procedure based 
on estimation of workflow cost does not degrade the solution, but rather in 
many cases it enables larger workflows that would lead to budget overrun to
be rejected. Thus it can save the space for smaller workflows to complete.

%Fig.~\ref{fig:score} shows the score $s$ as defined in Eq.~\ref{eq:score}
%computed for two sample applications. Comparing
%Fig.~\ref{fig:number-complete-pareto} to Fig.~\ref{fig:score} we can see that
%the latter one gives more flattened shape: this means that e.g. the improvement
%of the solution by one workflow may mean adding a one low-priority workflow.
%Nevertheless we can see that the curves on both figures have a very similar
%shape, which mens that our algorithms obey the priority rules and do not try to
%admit lower-priority workflows at the cost of the more important ones.

\begin{table}[tb]
\centering

(a) Number of workflows completed $N_c$:
\medskip
\begin{tabular}{r|cccccc}
 & M & C & L & E & S & All\tabularnewline
\hline
DPDS      &   57  & 45 &  18 &  14  &  0 & 134\tabularnewline
WA-DPDS   &    96 &  87  & 48  & 34  &  0 & 265\tabularnewline
SPSS     &    29  & 42  & 192  & 74  & 50 & 387\tabularnewline
\end{tabular}
\medskip

(b) Total amount of computing time completed $T_C$:
\medskip
\begin{tabular}{r|cccccc}
 & M & C & L & E & S & All\tabularnewline
\hline
DPDS      &   58  & 45 &  19 &  14  &  0 & 136\tabularnewline
WA-DPDS   &    90  & 96  & 57  & 27 &   8 & 278\tabularnewline
SPSS     &    31 &  18 &  170  & 75 &  42 & 336\tabularnewline
\end{tabular}
\medskip

(c) Exponential score $s$:
\medskip
\begin{tabular}{r|cccccc}
 & M & C & L & E & S & All\tabularnewline
\hline
DPDS      &   58  & 45 &  19 &  14  &  0 & 136\tabularnewline
WA-DPDS   &    90  & 96  & 57  & 27 &   8 & 278\tabularnewline
SPSS     &    31 &  18 &  170  & 75 &  42 & 336\tabularnewline
\end{tabular}
\medskip

\caption{Summary table for Pareto-distributed workflows showing number of times
each algorithm achieved the highest score over all budgets and deadlines. Data from a total of 525 runs on 
Pareto-distributed ensembles for Montage (M), CyberShake (C), LIGO (L), 
Epigenomics (E), SIPHT (S).
\label{tab:num-dags-pareto}}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[tb]
\centering

(a) Number of workflows completed $N_c$:
\medskip
\begin{tabular}{r|cccccc}
 & M & C & L & E & S & All\tabularnewline
\hline
DPDS      &   78 &  93 & 161 &  24  & 30 & 386\tabularnewline
WA-DPDS   &    78  & 97 & 179  & 58  & 28 & 440\tabularnewline
SPSS     &    78  & 79 & 195 &  75 &  50 & 477\tabularnewline
\end{tabular}
\medskip

(b) Total amount of computing time completed $T_C$:
\medskip
\begin{tabular}{r|cccccc}
 & M & C & L & E & S & All\tabularnewline
\hline
DPDS      &   78 &  93 & 161 &  24 &  30 & 386\tabularnewline
WA-DPDS   &    78  & 97 & 179 &  46  & 28 & 428\tabularnewline
SPSS     &    78 &  77 & 195  & 75 &  50 & 475\tabularnewline
\end{tabular}
\medskip

(c) Exponential score $s$:
\medskip
\begin{tabular}{r|cccccc}
 & M & C & L & E & S & All\tabularnewline
\hline
DPDS      &   78  & 93 & 161 &  24 &  30 & 386\tabularnewline
WA-DPDS   &    78 &  97 & 179  & 46  & 28 & 428\tabularnewline
SPSS     &    78 &  77 & 195 &  75  & 50 & 475\tabularnewline
\end{tabular}
\medskip

\caption{Summary table for equal size workflows showing number of times each
algorithm achieved the highest score over all budgets and deadlines. Data from a total of 525 runs on 
ensembles of equal size workflows for Montage (M), CyberShake (C), LIGO (L), 
Epigenomics (E), SIPHT (S).
\label{tab:num-dags-equal}}
\end{table}

Fig.~\ref{fig:total-time} presents the same results using the total computing
time metric, $T_C$. These plots confirm our previous observations about the
algorithms' performance, but they also allow us observe a more general property:
for a given budget, when the deadline is tightened the amount of work done
steeply decreases. Assigning a shorter deadline means that more VMs need to be
provisioned to complete the work. However, because of the dependencies in the 
workflows, many VMs will either be idle, or will be backfilled with tasks from 
low-priority workflows that never finish, resulting in reduced resource 
utilization. Similarly, the only way to reduce cost when adding VMs is to 
achieve a super-linear speedup, which is unlikely in practice. Therefore, if 
the deadline permits, and cost is important, parallelism should be kept to a 
minimum.

These observations are further confirmed when examining the effective computing
cost, as shown in Fig.~\ref{fig:cost}. It can be seen that lower resource
utilization and lower parallel efficiency for short deadlines lead to
substantial increases in the effective computing cost. This increase can be 
significant. For example, a SIPHT ensemble with short deadlines and a budget
of \$1,000 has an effective computing cost $\sim$8--20\% higher than the 
normal cost, depending on the algorithm used. That means that out of the 
\$1,000 budget, \$80--\$200 is spent on idle resources.

Another general conclusion can be drawn from Fig.~\ref{fig:cost}: despite some
random fluctuations resulting mostly from the finite result space, all the
curves for all the applications and budgets have a similar shape. This shape
represents a trade-off curve between two conflicting objectives: time and cost.
We observe that the selection of deadline and cost can be formulated as a
multi-criteria optimization problem, and that the curves presented in 
Fig.~\ref{fig:cost} are approximations of the Pareto front (or Pareto set) of
solutions to this problem. In our case the solutions were achieved by maximizing
the amount of work done, which directly results in minimization of effective 
computing cost metric for a given deadline. Therefore, the obtained results 
can be used to aid in resource allocation planning, when both cost and time 
criteria are given not as constraints but as objectives.

% \begin{figure*}[htb] \centering
% \includegraphics[width=0.19\textwidth]{figures/score2-MONTAGE-n-1000-8-dagh1-20m0.pdf}
% \includegraphics[width=0.19\textwidth]{figures/score2-CYBERSHAKE-n-1000-8-dagh1-20m0.pdf}
% \includegraphics[width=0.19\textwidth]{figures/score2-LIGO-n-1000-8-dagh1-40m0.pdf}
% \includegraphics[width=0.19\textwidth]{figures/score2-GENOME-n-1000-8-dagh100-1500m0.pdf}
% \includegraphics[width=0.19\textwidth]{figures/score2-SIPHT-n-1000-8-dagh5-50m0.pdf}
% \caption{Comparison of DPDS, AW-DPDS and SPSS algorithms for ensembles of 100
% Pareto-distributed workflows. Score is computed as $\sum(2^(-p))$ where
% priorities of subsequent workflows are $p=0,1,2,\ldots$ (0 means highest
% priority).} \label{fig:} \end{figure*}
       
\begin{figure}[htb] \centering
\includegraphics[width=0.7\columnwidth]{figures/pareto-budget-SIPHT-n-1000-8-dagh5-50h10m0.pdf}\\
(a) ensemble of Pareto-distributed workflows
\includegraphics[width=0.7\columnwidth]{figures/constant-budget-SIPHT-n-1000-0-dagh5-50h4m0.pdf}\\
(b) ensemble of workflows of equal size \caption{Comparison of number of
workflows completed for a given deadline, plotted versus different budget
amounts.}
\label{fig:budgets}
\end{figure}
               
In order to compare the efficiency of our algorithms, we counted the number of times
each algorithm achieved the best result for all simulated applications, deadlines
and budgets. Table~\ref{tab:num-dags-pareto} shows these summary values for 
Pareto-distributed workflows. Note that all algorithms that achieve the best 
result are credited, so the sum for a given application can be greater than the
total number of scenarios. These results depend on the metric: number of
workflows completed does not count the priorities or sizes of workflows, which
means that it happens more often that all the algorithms achieve the same best
result. However, it does not mean that all algorithms complete the same
workflows: that is why the values in (a), (b) and (c) are different. The 
exponential score and total amount of computing time completed give the same
rankings, since in our test ensembles larger workflows were given higher
priorities. It can be observed based on the sampled problem space that the
workflow-aware algorithms achieve the highest score in $\sim$2--3 times more 
cases than the simple DPDS algorithm, which does not take into account workflow 
structure or size. SPSS performs the best in $\sim$20\% more cases than WA-DPDS,
including the Montage and CyberShake ensembles, for which SPSS does not 
perform as well as WA-DPDS.

Similar results were achieved for ensembles consisting of workflows of equal
size. A summary of these results is shown in Table~\ref{tab:num-dags-equal}.
Again, we can see that the static SPSS algorithm performed better for coarse-grained
workflows (LIGO, Epigenomics, SIPHT). The differences between algorithm scores
are smaller since the workflows have equal size, therefore rejecting a particular
workflow does not enable the algorithms to complete any other workflows.

Fig.~\ref{fig:budgets} shows the same data for the SIPHT ensemble from another
perspective. It helps answer the question: Given a deadline, how many workflows
can be finished when we increase the budget? In Fig.~\ref{fig:budgets}(a) the
growth is super-linear, which results from the Pareto-like distribution of 
workflow sizes in the ensemble (see Fig.~\ref{fig:ensemble-distribution}). 
For ensembles of constant size workflows (see Fig.~\ref{fig:budgets}(b)) 
the growth is nearly linear.



\section{Conclusion and Future Work}
\label{sec:conclusions}

In this paper we addressed the interesting and important new problem of 
scheduling and resource provisioning for scientific workflow ensembles 
on IaaS clouds. The problem is different from previous work on grid and 
utility grid scheduling in that cloud infrastructures provide more control 
over the resources, so the resource provisioning plan can be adjusted 
according to the requirements of the application. Therefore the problem 
space becomes larger: it requires not only an efficient mapping of tasks
to available resources, but also the selection of the best resource 
provisioning plan. Formulating the problem as a maximization of the
number of prioritized workflows completed from the ensemble is also novel and  
requires workflows to be admitted or rejected based on their estimated 
resource demands. We believe that this bi-constrained problem is highly 
relevant because such constraints are commonly imposed on many real-world 
projects. The approach is also directly applicable to grid environments 
that provide resource reservations and charge service units for resource use.

We have considered both static and dynamic scheduling approaches and 
developed the DPDS, WA-DPDS and SPSS algorithms based on these 
strategies. The algorithms were evaluated on ensembles of synthetic 
workflows, which were generated based on real scientific applications. For 
the purposes of evaluation we have developed a simulator that models: the 
cloud infrastructure, and a workflow engine with tightly-coupled scheduler 
and provisioner modules. The results of our simulation studies indicate 
that the two algorithms that take into account information about the 
structure of the workflow and task runtime estimates (WA-DPDS and SPSS) 
yield better results than the simple on-line priority-based scheduling 
strategy (DPDS).

During our work, several interesting lessons were learned about running 
workflow ensembles on cloud infrastructures. One result is that the simple 
strategy used by DPDS performs relatively well in terms of the number of 
workflows completed and the high resource utilization it can achieve. However,
the extra workflows may be of lower-priority, and the high utilization
may be achieved by running tasks from workflows that cannot be completed
given the constraints. On the other hand, DPDS may be the only choice when 
estimates of task runtimes are not available in advance. We also observed
that the WA-DPDS and SPSS algorithms are able to find better quality 
solutions in terms of the target metrics: this is primarily a result of the 
admission algorithms rejecting workflows that are too large to be completed 
given the constraints, and accepting more smaller workflows that can be 
completed.

Another general observation is that although IaaS clouds provide elasticity
and the illusion of unlimited resources, which makes it possible to provision
more VMs to run workflows in parallel, it turns out that higher resource
utilization and better cost effectiveness can be achieved by limiting 
parallelism. This observation comes from the fact that 
starting and stopping VMs incurs overheads which can be amortized by running
each VM for as long as possible. This effect was confirmed in the case of 
the SPSS algorithm for Montage and CyberShake workflows with small budgets.
In these cases SPSS decided to provision many VMs in parallel, and shut them down early,
so that tasks early in the ensemble workflows could be completed by their 
deadlines. Similarly, a high degree of parallelization leads to lower 
resource utilization and increased cost. This result, visible in 
Fig~\ref{fig:cost}, leads to multi-criteria decision problems and related 
trade-offs. E.g. our simulation results can be used as hints to answer
the question how much time and money is required to actually complete all the
workflows from the given ensemble.

Another lesson learned from our studies is the effect of task granularity on 
workflow ensembles. All the benchmark workflows that we tested have an average 
number of tasks (50--1000), but the Montage and CyberShake workflows have 
fine-grained tasks with execution times on the order of seconds to minutes, 
while LIGO, Epigenomics and SIPHT have coarser-grained tasks with runtimes 
ranging from several minutes to hours. We observed that the dynamic 
algorithms achieved better results on the fine grained workflows since the 
simple random scheduling strategy leads to good load-balancing and high 
resource utilization for these ensembles. On the other hand, when tasks are
longer, it becomes more beneficial to apply the more sophisticated static 
scheduling strategy of SPSS.

Our current study suggests many areas for future work. One interesting
issue that we plan to investigate is how the static and dynamic algorithms 
perform with uncertain task runtimes and failures, which are known to have 
an important influence on overall system performance~\cite{Sakellariou2010,Dongarra2007}. 
We also plan to extend our application and infrastructure model to explicitly 
include the various data storage options available on clouds (right now we only 
model data access as part of the task time and do not include data costs). A previous 
experimental study on that subject~\cite{Juve2010} suggests that the data 
demands of scientific workflows have a large impact on not only the 
execution time, but also on the cost of workflows in commercial clouds. 
Finally, we plan to investigate heterogeneous environments that include 
multiple VM types and cloud providers, including private and community clouds, 
which will make the problem even more complex and challenging~\cite{Marshall2010,vockler11,Juve2010}.


%\section{Acknowledgments}
%Acknowledgements go here.

\bibliographystyle{abbrv}
\bibliography{paper}

%\appendix
%\section{Headings in Appendices}

%\balancecolumns

\end{document}
