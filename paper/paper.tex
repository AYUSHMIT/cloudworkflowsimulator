\documentclass[preprint,5p]{elsarticle}
%\documentclass[final,1p,times,twocolumn]{elsarticle}
\usepackage{fancyhdr}
\usepackage{graphicx}
\graphicspath{{./figures/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage[cmex10]{amsmath}
\usepackage{algorithm,algpseudocode}
\usepackage{hyperref} % must come before url
\usepackage{url}
\usepackage{color} % for \textcolor
\usepackage{subfig} % for \subfloat

\newcommand{\TODO}[1]{
  {\Large \textcolor{red}{\textbf{TODO: }#1}}
}

\journal{Future Generation Computer Systems}

\begin{document}

\begin{frontmatter}

% can use linebreaks \\ within to get better formatting as desired
\title{Algorithms for Cost- and Deadline-Constrained Provisioning for Scientific Workflow
Ensembles in IaaS Clouds}

\author[agh]{Maciej Malawski}
\ead{malawski@agh.edu.pl}
\author[isi]{Gideon Juve}
\ead{gideon@isi.edu}
\author[isi]{Ewa Deelman}
\ead{deelman@isi.edu}
\author[nd]{Jarek Nabrzyski}
\ead{naber@nd.edu}

\address[agh]{AGH University of Science and Technology, Dept. of Computer Science, Al. Mickiewicza 30, Krakow, Poland} 
\address[isi]{USC Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA, USA} 
\address[nd]{University of Notre Dame, Center for Research Computing, 111 ITC, Notre Dame, IN, USA}

\begin{abstract}

Large-scale applications expressed as scientific workflows are often grouped
into ensembles of inter-related workflows. In this paper, we address a new and
important problem concerning the efficient management of such ensembles under
budget and deadline constraints on Infrastructure- as-a-Service (IaaS) clouds.
IaaS clouds are characterized by on-demand resource provisioning capabilities
and a pay-per-use model. We discuss, develop, and assess algorithms based on
static and dynamic strategies for both task scheduling and resource
provisioning. We perform the evaluation via simulation using a set of scientific
workflow ensembles with a broad range of budget and deadline parameters, taking
into account task granularity, uncertainties in task runtime estimations,
provisioning delays, and failures. We find that the key factor determining the
performance of an algorithm is its ability to decide which workflows in an
ensemble to admit or reject for execution. Our results show that an admission
procedure based on workflow structure and estimates of task runtimes can
significantly improve the quality of solutions.

\end{abstract}

\begin{keyword}
scientific workflows; workflow ensembles; cloud computing; resource provisioning
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}

Large-scale scientific workflows, usually represented as Directed Acyclic Graphs
(DAGs), are an important class of applications that lead to challenging problems
in resource management on grid and cloud infrastructures. In addition, workflows
for large computational problems are often composed of several inter-related
workflows grouped into {\em ensembles}. Workflows in an ensemble typically have
a similar structure, but they differ in their input data, number of tasks, and
individual task sizes.

There are many applications that require scientific workflow ensembles.
CyberShake~\cite{Callaghan2011}, for example, uses ensembles to generate seismic
hazard maps.  Each workflow in a CyberShake ensemble generates a hazard curve
for a particular geographic location, and several hazard curves are combined to
create a hazard map.  In a 2013 study, CyberShake was used to generate a set
of hazard maps over 286 sites that required an ensemble of 2288
workflows~\cite{CyberShake13.4}. Similarly, users of Montage~\cite{Deelman2008}
often need several workflows  with different parameters to generate a set of
image mosaics that can be combined  into a single, large mosaic. The Galactic
Plane ensemble~\cite{GalacticPlane}, which generates several mosaics of the
entire sky in different wavelengths, consists of 17 workflows,
each of which contains 900 sub-workflows.  Another ensemble example is the
Periodograms application~\cite{Vockler2011}, which searches for extrasolar
planets by detecting periodic dips in the light intensity of their host star.
Due to the large scale of the input data, this application is often split up
into multiple batches processed by different workflows. Additional workflows are
created to run the analysis using different parameters. A recent analysis of
Kepler satellite data required three ensembles of 15 workflows.

Workflows in an ensemble may differ not only in their parameters, but also in
their priority. For example, in CyberShake some geographic locations may be in heavily
populated areas or have sensitive facilities such as power plants, while others
may be less important. Scientists typically prioritize the workflows in such
an ensemble so that important workflows are finished first. This enables them
to see critical results early, and helps them to choose the most important
workflows when the time and financial resources available for computing are
limited.

Infrastructure-as-a-Service (IaaS) clouds offer the ability to provision 
resources on-demand according to a pay-per-use model. These systems 
are regarded by the scientific community as a potentially attractive source of 
low-cost computing resources~\cite{Ostermann2010, Keahey2009}. In contrast 
to clusters and grids, which typically offer best-effort quality of service, clouds 
give more flexibility in creating a controlled and managed computing environment.
Clouds provide the ability to adjust resource capacity according to the changing
demands of the application, often called auto-scaling. However, giving users 
more control also requires the development of new methods for task 
scheduling and resource provisioning. Resource management decisions required 
in cloud scenarios not only have to take into account performance-related 
metrics such as workflow makespan or resource utilization, but must also consider
budget constraints, since the resources from commercial clouds
usually have monetary costs associated with them~\cite{Durkee2010}.

In this paper, we aim to gain insight into resource management challenges when
executing scientific workflow ensembles on clouds. We address a
new and important problem of maximizing the number of completed workflows from
an ensemble under both budget and deadline constraints. The motivation for this
work is to answer the fundamental question of concern to a researcher: {\em How much
computation can be completed given the limited budget and timeframe of a
research project?} 

The main contributions of this paper are:
\begin{itemize}
  \item we define the problem of scheduling prioritized workflow ensembles
  under budget and deadline constraints,
  \item we analyze and develop several dynamic (online) and static (offline) 
  algorithms for task scheduling and resource provisioning that rely on workflow 
  structure information (critical paths and workflow levels) and estimates of 
  task runtimes,
  \item we evaluate these algorithms using a simulator based on 
  CloudSim~\cite{Calheiros2011}, which models the infrastructure and the 
  application, taking into account uncertainties in task runtime estimates, 
  provisioning delays, and failures, 
  \item we discuss the performance of the algorithms on a set of
  synthetic workflow ensembles based on important, real scientific
  applications, using a broad range of different application scenarios and
  varying constraint values.
\end{itemize}

This paper is an extension of our earlier conference
publication~\cite{Malawski-SC12}.
Here we present a more detailed performance evaluation, including the discussion
of results based on varying deadline and budget constraints. We also report on new
results and discussion of task granularity in the context of the resource
billing cycle, which is an important aspect of scheduling and resource provisioning
on cloud infrastructures.


The paper is organized as follows: after an analysis of related work in
Section~\ref{sec:related}, Section~\ref{sec:problem} describes the
infrastructure and application model we are targeting in this paper. In
Section~\ref{sec:algorithms} we describe the dynamic and static algorithms we
developed. Section~\ref{sec:performance} presents test scenarios and performance
metrics, while Section~\ref{sec:results} discusses the results of simulation
studies of these algorithms. Finally, general conclusions, lessons learned, and
future work are outlined in Section~\ref{sec:conclusions}.


\section{Related Work}
\label{sec:related}
General policy-based and rule-based approaches to dynamic provisioning (such as Amazon
Auto Scaling~\cite{Autoscaling} and
RightScale~\cite{RightScale}) allow the size
of a resource pool to be adjusted based on infrastructure and
application metrics. A typical infrastruc\-ture-specific metric is system load,
whereas application-speci\-fic metrics include response time and length of a task or
of a request queue. It is possible to set thresholds and limits to tune the behavior
of these autoscaling systems, but no support for complex applications is provided.


Policy-based approaches for scientific workloads (e.g. \cite{Marshall2010,
Kim2011,Chen2011}) also allow to scale the cloud resource pool or to extend the
capabilities of clusters using cloud-burst techniques. Our approach is different
in that we consider workflows, while policy based approaches typically consider
bags of independent tasks or unpredictable batch workloads. This enables us to
take advantage of workflow-aware heuristics that cannot be applied to
independent tasks.


Our work is related to the strategies for deadline-con\-strained
cost-minimization workflow scheduling, developed for utility grid and cloud
systems. However, our problem is different from
\cite{Yu2005},\cite{Abrishami2013158} and~\cite{Young2013} in that we consider
ensembles of workflows instead of individual workflows.
Our work is also different from cloud-targeted autoscaling
solution~\cite{Mao2011} in that we consider ensembles of workflows rather than
unpredictable workloads containing workflows. We also consider budget
constraints rather than cost minimization as a goal. In other words, we assume
that there is more work to be done than the available budget, so some work must
be rejected.
Therefore, cost is not something we optimize (i.e. an objective), but rather a
constraint.


This work is related to bi-criteria scheduling and multi-criteria scheduling of
workflows~\cite{Wieczorek2009,Prodan2010,Dongarra2007,Durillo2013}. These approaches are
similar to ours in that we have two scheduling criteria: cost and makespan. The
challenge in multi-criteria scheduling is to derive an objective function that
takes into account all of the criteria. In our case one objective (amount
of work completed) is subject to optimization, whereas time and cost are
treated as constraints, which is not addressed by other works.


Our work can also be regarded as an extension of the budget-constrained 
workflow scheduling~\cite{Sakellariou2007}, the cost optimization for workflows 
under deadline constraints~\cite{Yu2005}, 
or methods of stretching and compacting the workflow to optimize time and cost~\cite{Lee2013}, 
in the sense that we are dealing with workflow ensembles
and the deadline constraint is added. 
Interesting discussion of uncertainties and the way to address them using Monte Carlo approach 
is also presented in~\cite{Zheng2013b}, where the objective is makespan optimization.  

Multiple workflows has been also considered in previous research, 
either in the context of heterogeneous systems~\cite{Zhao2006} 
where makespan and fairness optimization is applied, 
or in the context of grids~\cite{Hirales2010} 
where multiple heuristics are evaluated with respect to the makespan of schedules they produce. 
Scheduling multiple workflows in~\cite{Duan2012,Duan2014} applies multiobjective optimization 
of time and cost under storage bandwidth and capacity constraints with the use of heuristics based on game theory. 
The approach proposed in~\cite{TolosanaCalasanz20121300} focuses on enforcement of QoS 
measured as throughput of processing by individual workflows. 
The problem of workflow admission addressed in~\cite{Zheng2013a} is similar to the one solved by our algorithms, 
but the difference is that we address the problem from the end-user perspective, 
while in their work they assume that the resource provider decides 
whether to admit or reject the workflow based on the resource estimates and the deadline constraints.

Other approaches for scheduling workflows on grids and clouds such as ~\cite{Talukder2009,Pandey2010} use
metaheuristics that usually run for a long time before producing good results,
which makes them less useful in the scenarios we consider in this paper. 

Noteworthy are also methods based on mathematical programming, 
using linear programming or mixed-integer programming. 
Such method is applied to scheduling workflows on hybrid clouds using time discretization~\cite{Genez13b}, 
but the model presented there is limited to small scale workflows. 
Large-scale applications on hybrid clouds are addressed in~\cite{VandenBossche2010,VandenBossche2013973}, 
but their model considers bag-of-task applications instead of workflows. 
The cloud bursting scenario where private cloud is combined with a public one, described in~\cite{BittencourtM11}, 
addresses workflows, and the goal is to minimize the cost while keeping the deadline. 
In our related paper~\cite{optimization-ppam13}, we also use the mixed-integer programming approach 
to schedule multi-level deadline-constrained workflows, but workflow ensembles are not considered.

\begin{table*}[tb]
    \centering
    \caption{Summary table of related work on workflow scheduling and provisioning.} 
    \label{tab:related}
    \includegraphics[width=\textwidth]{Cloud-workflows-table}
\end{table*}



The analysis of related work is summarized in Table~\ref{tab:related}, where we classify the methods based on 7 categories. 
Infrastructure type can be grid or cloud, where grid includes also utility grids 
and heterogeneous computing systems where cost can be also relevant (such as in~\cite{Sakellariou2007}). 
Application types of interest are bags of tasks, single workflows or multiple workflows. 
The workload can be static, where the scheduling process can plan the tasks in advance, 
or dynamic, where the algorithm has to deal with unpredictable stream of jobs arriving. 
The provisioning methods are divided into dynamic and static approaches, 
where static means that the provisioning plan is prepared prior to execution. 
For simplification, we classify all the algorithms for grid and heterogeneous systems into the static category, 
since these systems do not allow for creation of new resources on demand, 
even if some mechanisms for dynamic provisioning such as pilot jobs are available. 
The related work addresses multiple optimization objectives, 
including time, monetary cost, reliability, energy or fairness, 
and there are examples of multicriteria optimization among them. 
Most of the scheduling approaches consider also constraints for optimization, 
and most commonly addressed ones are budget and deadline, 
whereas such additional constraints as bandwidth, storage capacity or job throughput are also present. 
Finally, we divide the algorithmic methods into heuristics, 
including various list scheduling techniques based on graph analysis, 
metaheuristics that use genetic or evolutionary algorithms, 
and the approaches using mathematical programming such as linear 
or mixed-integer programming for which ready to use solvers exist.

The analysis of the table suggests that there are many approaches to workflow scheduling 
and resource provisioning on clouds, but there is still room for improvement. 
New infrastructure types, including hybrid and federated clouds should be investigated, 
more emphasis should be put on problems with multiple workflows, 
and new methods for combining static and dynamic workloads and algorithms offer potentially interesting area of research. 
Finally, other objectives and constraints in addition to budget and execution time can be explored. 

In this context, we consider our attempt to address workflow ensembles using both static and dynamic algorithms 
presented in this paper an interesting and original contribution that has not been explored before.    


 
\section{Problem Description}
\label{sec:problem}

\subsection{Resource Model}  
We assume a resource model similar to Amazon's Elastic Compute Cloud (EC2),
where virtual machine (VM) instances may be provisioned on-demand and are billed by
the hour, with partial hours being rounded up. Although there may be
heterogeneous VM types with different amounts of CPU, memory, disk space, and
I/O, for this paper we focus on a single VM type because we assume that for most
applications there will typically be only one or two VM types with the best
price/performance ratio for the application~\cite{Juve2009}. We assume that
a submitted task has exclusive access to a VM instance and that there is no
preemption. We also assume that there is a delay between the time that a new VM
instance is requested and when it becomes available to execute tasks.

\subsection{Application Model} 
The target applications are ensembles of
scientific workflows that can be modeled as DAGs, where the nodes in the graph
represent computational tasks, and the edges represent data- or control-flow
dependencies between the tasks. We assume that runtime estimates for the
workflow tasks are known, but that they are not perfect and may vary based on
a uniform distribution of $\pm p\%$.

This study uses synthetic workflows that were generated using historical data
from real applications~\cite{Bharathi2008}. The applications come from a wide
variety of domains including: bioinformatics (Epigenomics, SIPHT: sRNA
identification protocol using high-throughput technology), astronomy
(Montage), earthquake science (CyberShake), and physics (LIGO). The synthetic
workflows were generated using code developed in~\cite{WorkflowGenerator},
with task runtimes based on distributions gathered from running real
workflows.

Although workflows are often data-intensive, the algorithms described
here do not currently consider the size of input and output data
when scheduling tasks. Instead, it is assumed that all workflow data is stored in
a shared cloud storage system, such as Amazon S3, and that intermediate data
transfer times are included in task runtimes. It is also assumed that
data transfer times between the shared storage and the VMs are equal for
different VMs so that task placement decisions do not impact the
runtime of the tasks.

We assume that each workflow in an ensemble is given a numeric priority that
indicates how important the workflow is to the user. As such, the priorities
indicate the utility function of the user. These priorities are absolute in
the sense that completing a workflow with a given priority is more valuable
than completing all other workflows in the ensemble with lower priorities
combined. The goal of the workflow ensemble scheduling and cloud provisioning
problem is to complete as many high-priority workflows as possible given a
fixed budget and deadline. Only workflows for which all tasks are finished by
the deadline are considered to be complete---partial results are not usable in
this model.

\subsection{Performance Metric}
\label{sec:perf_metric}
In order to precisely define the objective of the algrithms it is necessary to
introduce a metric that can be used to score the performance of the different
algorithms on a given problem (ensemble, budget, and deadline). The simplest
approach is to count the number of workflows in the ensemble that each algorithm
is able to complete within the budget before the deadline, but this metric does
not account for the priority-based utility function specified by the user. Using
the counting approach, a less efficient algorithm may be able to complete a
large number of low-priority workflows by executing the smallest workflows
first. In order to account for the priority, we use an exponential
scoring defined as:
\begin{equation}
\label{eq:score}
Score(e) = \sum_{w~\in~Completed(e)}{2^{-Priority(w)}}
\end{equation}
where $Completed(e)$ is the set of workflows in ensemble $e$ that was completed
by the algorithm, and $Priority(w)$ is the priority of workflow $w$ such that
the highest-priority workflow has $Priority(w)=0$, the next highest workflow has
$Priority(w)=1$, and so on. This exponential scoring function gives the highest
priority workflow a score that is higher than all the lower- priority workflows
combined:
$$ 2^{-p} > \sum_{i~=~p+1,~\ldots}2^{-i} $$
This scoring is consistent with our definition of the problem, which is to
complete as many workflows as possible, according to their priorities, given a
set budget and deadline.


\section{Algorithms}
\label{sec:algorithms}

This section describes three algorithms that were developed to schedule and
provision resources for ensembles of workflows on the cloud under budget and
deadline constraints.

\begin{algorithm}[tb]
\caption{Dynamic provisioning algorithm for DPDS}
\label{alg:prov}
{\footnotesize
\begin{algorithmic}[1]
\Require $c$: consumed budget; $b$: total budget; $d$: deadline; $p$: price;
$t$: current time; $u_h$: upper utilization threshold; $u_l$: lower utilization
threshold; $v_{max}$: maximum number of VMs
\Procedure{Provision}{}
  \State $V_R\gets$ set of running VMs
    \State $V_C\gets$ set of VMs completing billing cycle
    \State $V_T\gets \emptyset$ \Comment{set of VMs to terminate}
    \State $n_T\gets 0$ \Comment{number of VMs to terminate}
    \If{$ b - c < |V_C| * p\ or\ t > d $ }
      \State $n_T\gets |V_R| - \lfloor(b-c)/p\rfloor$
      \State $V_T\gets$ select $n_T$ VMs to terminate from $V_C$
      \State \Call{Terminate}{$V_T$} \label{l:terminate1}
    \Else 
    \State $u\gets$ current VM utilization
      \If{$u>u_h$ and $|V_R| < v_{max}*N_{VM}$}
        \State \Call{Start}{$new\ VM$}
      \ElsIf{$u<u_l$}
        \State $V_I\gets$ set of idle VMs
        \State $n_T\gets \lceil|V_I|/2\rceil$ \label{l:nT2}
      \State $V_T\gets$ select $n_T$ VMs to terminate from $V_I$
        \State \Call{Terminate}{$V_T$} \label{l:terminate2}
      \EndIf 
    \EndIf
\EndProcedure
\end{algorithmic}}
\end{algorithm}

\subsection{Dynamic Provisioning Dynamic Scheduling (DPDS)}
\label{sec:dpds}

DPDS is an online algorithm that provisions resources and schedules tasks at
runtime. It consists of two main parts: a provisioning procedure, and a
scheduling procedure.

DPDS' provisioning procedure is based on resource utilization. DPDS starts with
a fixed number of resources calculated based on the available time and budget,
and adjusts the number of resources according to how well they are utilized by
the application.
Given a budget in dollars $b$, deadline in hours $d$, and the hourly price of a
VM in dollars $p$, it is possible to calculate the number of VMs, $N_{VM}$, to
provision so that the entire budget is consumed before the deadline:
%
\begin{equation}
\label{eq:static-plan}
N_{VM} = \lceil b / (d * p) \rceil
\end{equation}
%
DPDS provisions $N_{VM}$ VMs at the start of the ensemble execution, then it
periodically computes resource utilization using the percentage of idle VMs
over time and adjusts the number of VMs if the utilization is above or below
given thresholds. Because it is assumed that VMs are billed by the hour, DPDS
only considers VMs that are approaching their hourly billing cycle when
deciding which VMs to terminate. This dynamic provisioning algorithm is shown
in Algorithm~\ref{alg:prov}.

\begin{algorithm}[tb]
\caption{Priority-based scheduling algorithm for DPDS}
\label{alg:ds}
{\footnotesize
\begin{algorithmic}[1]
\Procedure{Schedule}{}
    \State $P\gets$ empty priority queue
  \State $IdleVMs\gets$ set of idle VMs
  \For{root task $t$ in all workflows}
      \State \Call{Insert}{$t,P$} 
    \EndFor
    \While{deadline not reached}
      \While{$IdleVMs \neq \emptyset\ and\ P \neq \emptyset$}
        \State $v\gets$ \Call{SelectRandom}{$IdleVMs$}
        \State $t\gets$ \Call{Pop}{$P$}
        \State \Call{Submit}{$t,v$}
      \EndWhile
      \State Wait for task $t$ to finish on VM $v$
      \State Update $P$ with ready children of $t$
    \State \Call{Insert}{$v,IdleVMs$}
    \EndWhile
\EndProcedure
\end{algorithmic}
}
\end{algorithm}

The set of VMs completing their billing cycle is determined by both
the provisioner interval, and the termination delay of the provider. This
guarantees that VMs can be terminated before they start the next billing cycle
and prevents the budget from being overrun. The VMs terminated in line
\ref{l:terminate1} of Algorithm~\ref{alg:prov} are the ones that would overrun
the budget if not terminated in the current provisioning cycle. The VMs
terminated in line \ref{l:terminate2} are chosen to increase the resource
utilization to the desired threshold. In order to prevent instances that have
already been paid for from being terminated too quickly, no more than half of
the idle resources are terminated during each provisioning cycle. To avoid an
uncontrolled increase in the number of instances, which may happen in the case
of highly parallel workflows, the provisioner will not start a new VM if the
number of running VMs is greater than the product of $N_{VM}$ (from
Equation~\ref{eq:static-plan}) and an autoscaling parameter, $v_{max}$. Unless
otherwise specified, $v_{max}$ is assumed to be 1.

In order to schedule individual workflow tasks onto available VMs, DPDS uses the
dynamic, priority-based scheduling procedure shown in Algorithm~\ref{alg:ds}.
Initially, the ready tasks from all workflows in the ensemble are added to a
priority queue based on the priority of the workflow to which they belong. If
there are idle VMs available, and the priority queue is not empty, the next task
from the priority queue is submitted to an arbitrarily chosen idle VM. The
process is repeated until there are no idle VMs or the priority queue is empty.
The scheduler then waits for a task to finish, adds its ready children to the
priority queue, marks the VM as idle, and the entire process repeats until the
deadline is reached.



DPDS guarantees that tasks from lower priority workflows are always deferred
when higher-priority tasks are available, but lower-priority tasks can still
occupy idle VMs when higher-priority tasks are not available. Since there is
no preemption, long-running low-priority tasks may delay the execution of
higher-priority tasks. In addition, tasks from low priority workflows may be
executed even though there is no chance that those workflows will be completed
within the current budget and deadline. 


\subsection{Workflow-Aware DPDS (WA-DPDS)}

DPDS does not use any information about the structure of the
workflows in the ensemble when scheduling tasks.  It does not
consider whether a lower priority task belongs to a workflow that will never be
able to complete given the current budget and deadline. As a result, DPDS may
start lower priority tasks just to keep VMs busy that will end up delaying
higher priority tasks later on, making it less likely that higher priority
workflows will be able to finish.

The Workflow-Aware DPDS (WA-DPDS) algorithm extends DPDS by introducing a
workflow admission procedure, which is invoked whenever WA-DPDS sees the first
task of a new workflow at the head of the priority queue (i.e. when no other
tasks from the workflow have been scheduled yet). The admission procedure---
shown in Algorithm~\ref{alg:wa-dpds}---estimates whether there is enough
budget remaining to admit the new workflow; if there is not, then the workflow
is rejected and its tasks are removed from the queue. WA-DPDS compares the
current cost (consumed budget) and remaining budget, taking into account the
cost of currently running VMs, and the cost of workflows that have already
been admitted. In addition, it adds a small safety margin of \$0.10 (10\% of the
compute hour cost) to avoid going over budget. We found the admission procedure useful not only to prevent
low-priority workflows from delaying high-priority ones, but also to reject
large and costly workflows that would overrun the budget and admit smaller
workflows that can efficiently utilize idle resources in 
ensembles containing workflows of non-uniform sizes. It would also be possible 
to extend this admission procedure to check other constraints, such as whether
the estimated critical path of the new workflow exceeds the time remaining
until the deadline.

\begin{algorithm}[tb]
\caption{Workflow admission algorithm for WA-DPDS}
\label{alg:wa-dpds}
{\footnotesize
\begin{algorithmic}[1]
\Require $w$: workflow; $b$: budget; $c$: current cost
\Procedure{Admit}{$w,b,c$}
    \State $r_n\gets b-c$ \Comment{Budget remaining for new VMs}
    \State $r_c\gets $cost committed to VMs that are running
    \State $r_a\gets $cost to complete workflows previously admitted
  \State $r_m\gets 0.1$ \Comment{Safety margin}
  \State $r_b\gets r_n+r_c-r_a-r_m$ \Comment{Budget remaining}
  \State $c_w\gets$ \Call{EstimateCost}{w}
    \If{$c_w<r_b$}  \textbf{return} $TRUE$
    \Else ~  \textbf{return} $FALSE$
  \EndIf       
\EndProcedure
\end{algorithmic}
}
\end{algorithm}


\subsection{Static Provisioning Static Scheduling (SPSS)}

The previous dynamic (online) algorithms make provisioning and scheduling
decisions at runtime. By contrast, the SPSS algorithm creates a provisioning
and scheduling plan before running any workflow tasks. This enables SPSS to
start only those workflows that it knows can be completed given the deadline and
budget constraints, and eliminates any waste that may be allowed by the dynamic
algorithms.

The approach used by SPSS is to plan each workflow in the ensemble in priority
order, rejecting any workflows that would exceed the deadline or budget. Once the
plan is complete, the VMs are provisioned and tasks are executed according to
the schedules given by the plan.

The disadvantage of the static planning approach used by SPSS is that it is
sensitive to dynamic changes in the environment and the application that may
disrupt its carefully constructed plan. For example, if there are provisioning
delays, or if the runtime estimates for the tasks are inaccurate, then workflow
execution may diverge from the plan. This issue will be discussed further in
Sections~\ref{sec:variances} and \ref{sec:delays}.

Algorithm~\ref{alg:admit} shows how ensembles are planned in SPSS. Workflows
from the ensemble are considered in priority order. For each workflow, SPSS
attempts to build on top of the current plan by provisioning VMs to schedule the
tasks of the workflow so that it finishes before the deadline with the least
possible cost. If the cost of the new plan is less than the budget, then the new
plan is accepted and the workflow is admitted. If not, then the new plan is
rejected and the process continues with the next workflow in the ensemble. The
idea is that, if each workflow can be completed by the
deadline with the lowest possible cost, then the number of workflows that can be
completed within the given budget will be maximized.

To plan a workflow, the SPSS algorithm assigns sub-deadlines to each individual
task in the workflow, and then schedules each task so as to minimize the cost of
the task while still meeting its assigned sub-deadline. If each task can be
completed by its deadline in the least expensive way, then the cost of the
entire workflow can be minimized without exceeding the deadline. SPSS assigns
sub-deadlines to each task based on the slack time of the workflow, which is
defined as the amount of extra time that a workflow can extend its critical path
and still be completed by the ensemble deadline. For a workflow $w$, the slack
time of $w$ is: $ ST(w) = d - CP(w) $
where $d$ is the deadline and $CP(w)$ is the critical path of $w$. We
assume that $CP(w) \leq d$, otherwise the workflow cannot be completed by the
deadline and must be rejected. 
For large ensembles we expect the critical path
of any individual workflow to be much less than the deadline.


\begin{algorithm}[tb]
\caption{Ensemble planning algorithm for SPSS}
\label{alg:admit}
{\footnotesize
\begin{algorithmic}[1]
\Require $W$: workflow ensemble; $b$: budget; $d$: deadline
\Ensure Schedule as much of $W$ as possible given $b$ and $d$
\Procedure{PlanEnsemble}{$W,b,d$}
    \State $P\gets \emptyset$ \Comment{Current plan}
    \State $A\gets \emptyset$ \Comment{Set of admitted DAGs}
    \For{$w\ in\ W$}
        \State $P^\prime \gets$\ \Call{PlanWorkflow}{$w,P,d$}
        \If{$Cost(P^\prime) \le b$}
            \State $P\gets\ P^\prime$ \Comment{Accept new plan}
            \State $A \gets A\ +\ w$ \Comment{Admit w}
        \EndIf
    \EndFor
    \State \textbf{return} $P,A$
\EndProcedure
\end{algorithmic}
}
\end{algorithm}

A task's level is the length of the longest path between the task and an entry
task of the workflow:
%
$$
Level(t) = \begin{cases}
0,&$if~$Pred(t) = \emptyset\cr
\max_{p \in Pred(t)}Level(p)+1,&$otherwise.$
\end{cases}
$$
%
SPSS distributes the slack time of the workflow by level, so that each level
of the workflow gets a portion of the workflow's slack time proportional to
the number of tasks in the level and the total runtime of tasks in the level.
The idea is that levels containing many tasks and large runtimes should be
given a larger portion of the slack time so that tasks in those levels may be
serialized. Otherwise, many resources need to be allocated to run all of the
tasks in parallel, which may be more costly.

The slack time of a level $l$ in workflow $w$ is given by:
%
$$
ST(l) = ST(w) \Biggl[\left({\alpha}\frac{N(l)}{N(w)}\right) + \left({(1 - \alpha)}\frac{R(l)}{R(w)} \right)\Biggr]
$$
%
where $N(w)$ is the number of tasks in the workflow, $N(l)$ is the number of
tasks in level $l$, $R(w)$ is the total runtime of all tasks in the workflow,
$R(l)$ is the total runtime of all tasks in level $l$, and $\alpha$ is a
parameter between 0 and 1 that causes more slack time to be given to levels
with more tasks (large $\alpha$) or more runtime (small $\alpha$).

The deadline of a task $t$ is then:
%
\begin{equation}
\label{eq:spss-deadline}
DL(t) = LST(t) + RT(t) + ST(Level(t))
\end{equation}
%
where $Level(t)$ is the level of $t$, $RT(t)$ is the runtime of $t$, and 
$LST(t)$ is the latest start time of $t$ determined by:
%
$$
LST(t) = \begin{cases}
0,&$if~$Pred(t) = \emptyset\cr
\max_{p \in Pred(t)} DL(p),&$otherwise.$
\end{cases}
$$

\begin{algorithm}[tb]
\caption{Workflow planning algorithm for SPSS}
\label{alg:planworkflow}
{\footnotesize
\begin{algorithmic}[1]
\Require $w$: workflow; $P$: current plan; $d$: deadline
\Ensure Create plan for $w$ that minimizes cost and meets deadline $d$
\Procedure{PlanWorkflow}{$w,P,d$}
    \State $P^\prime\gets$ copy of $P$
    \State \Call{DeadlineDistribution}{w,d}
    \For{$t\ in\ w\ sorted\ by\ DL(t)$}
        \State $v \gets$ VM that minimizes cost and start time of t
        \If{$FinishTime(t,v) < DL(t)$}
            \State Schedule(t,v)
        \Else
            \State Provision a new VM v
            \State Schedule(t,v)
        \EndIf
    \EndFor
    \State \textbf{return} $P^\prime$
\EndProcedure
\end{algorithmic}
}
\end{algorithm}


Algorithm~\ref{alg:planworkflow} shows how SPSS creates low-cost plans for each
workflow. The \textsc{PlanWorkflow} procedure first calls
\textsc{DeadlineDistribution} to assign sub-deadlines to tasks.
Then, \textsc{PlanWorkflow} schedules
tasks onto VMs, allocating new VMs when necessary. For each task in the
workflow, the least expensive slot is chosen to schedule the task so that it can
be completed by  its deadline. VMs are allocated in blocks of one billing cycle
(one hour) regardless of the size of the task. When computing the cost of
scheduling a task on a given VM, the algorithm considers idle slots in blocks
that were allocated for previous tasks to be free, while slots in new blocks
cost the full price of a billing cycle. 
For example, if a task has a runtime of
10 minutes, and the price of a block is \$1, then the algorithm will either
schedule the task on an existing VM that has an idle slot larger than 10
minutes for a cost of \$0, or it will allocate a new block on an existing VM,
or provision a new VM, for a cost of \$1. 
If the cost of slots on two different
VMs is equal, then the slot with the earliest start time is chosen. To prevent too
many VMs from being provisioned, the algorithm always prefers to extend the
runtime of existing VMs before allocating new VMs. The result of this is that
the algorithm will only allocate a new block if there are no idle slots on
existing blocks large enough or early enough to complete the task by its
deadline, and it will only allocate a new VM if it cannot add a block to an
existing VM to complete the task by its deadline.



\subsection{Illustrative example}

To illustrate how the algorithms perform, we prepared an artificial ensemble 
of three workflows, as seen in Fig.~\ref{fig:example-ensemble}. 
The deadline is set to 6 hours and the budget to \$18. In this example we assume
that there are no VM provisioning delays, the runtime estimates are accurate, 
there are no failures and the cost of 1 VM-hour is \$1.
For dynamic algorithms we assume that the low utilization threshold ($u_l$) for deprovisioning is 50\%
and that the autoscaling parameter $v_{max} = 1$. 


\begin{figure}[htb] 
  \centering
  \includegraphics[width=0.80\columnwidth]{figures/example-ensemble}
  \caption{Artificial ensemble consisting of three workflows \{a, b, c\}, with priorities \{1, 2, 3\}.
  Task labels indicate estimated execution time in minutes, 
  so e.g. task $a.70$ has estimated execution time of 70 minutes.}
  \label{fig:example-ensemble}
\end{figure}

The resulting schedule produced by DPDS algorithm is shown in Fig.~\ref{fig:algorithms-example-dpds}.
The algorithm at the beginning estimates that it needs 3 VM instances by dividing the budget by the deadline ($18 / 6 = 3$). 
Next, it schedules all the ready tasks in the priority order, so $a.70$ is started first, 
and immediately $b.70$ and $c.60$ are started, since no other tasks of workflow a are ready. 
When task $a.60$ completes, there are again other no higher priority tasks ready, 
so the next task from workflow $c$ can be started ($c.45$). Since there is no preemption,
this results in workflow $a$ being delayed by the lower priority workflow $c$.
Finally, the outcome of this dynamic scheduling policy is that none of the workflows completes within the deadline.
Actually, the tasks $b.100$, $c.65$ and $a.160$ are not finished since all the VMs 
are terminated due to running out of budget. 
The exponential score achieved by the algorithm computed according to equation~\ref{eq:score} is 0.
On the other hand, we can observe that the utilization of the VMs is kept on the level of 100\% throughout the 
whole execution time, which may be considered an advantage in some cases.

\begin{figure}[htb] 
  \centering
  \includegraphics[width=0.80\columnwidth]{figures/algorithms-example-dpds}
  \caption{Schedule produced by DPDS algorithm for the example ensemble from Figure~\ref{fig:example-ensemble}.
  White boxes indicate VM instances provisioned for their hourly time slots, 
  and solid boxes indicate running tasks.
  None of the workflows completes within the deadline.}
  \label{fig:algorithms-example-dpds}
\end{figure}


The resulting schedule produced by WA-DPDS algorithm is shown in Fig.~\ref{fig:algorithms-example-wadpds}.
The schedule is generated in a similar way as in DPDS algorithm, with the one notable difference. 
After scheduling tasks $a.70$ and $b.70$, the algorithm estimates whether it has enough budget to 
admit workflow $c$. The answer is that based on the sum of task runtimes there is no room for workflow $c$,
so the whole workflow is rejected. As a consequence, task $a.100$ can start at time $t=70$ minutes
thus workflow $a$ is not delayed as in the case of DPDS. One can also observe that $VM.2$ and $VM.3$ remain idle for
one hour, which is the consequence of the conservative provisioning settings we assume in this example.  
This means that VM utilization at the level of 2/3 does not trigger deprovisioning in this case, and the maximum autoscaling
factor set to 1 prevents from provisioning of new VMs over the initial number even if utilization is 100\%.
In the resulting schedule only workflow $a$ completes, so the score is $2^{-1} = 0.5$, which is better than DPDS.

\begin{figure}[htb] 
  \centering
  \includegraphics[width=0.80\columnwidth]{figures/algorithms-example-wadpds}
  \caption{Schedule produced by WA-DPDS algorithm for the example ensemble from Figure~\ref{fig:example-ensemble}.
  Workflow a completes within the deadline.}
  \label{fig:algorithms-example-wadpds}
\end{figure}


The final schedule produced by the SPSS algorithm is shown in
Fig.~\ref{fig:algorithms-example-spss}. SPSS generates this schedule by
considering workflows in priority order. It first considers workflow $a$, and
develops a low-cost schedule that completes $a$ by its deadline using a minimum
number of VM-hours. This process involves assigning a deadline to each task
according to Equation~\ref{eq:spss-deadline}, and then scheduling tasks in
deadline-order on the VM that can complete each task by its deadline with the
the minimum increase in cost. Once $a$ has been scheduled, SPSS will develop an
updated low-cost schedule that includes workflow $b$. However, after adding $b$,
the algorithm will determine that the total cost of the combined schedule
for $a$ and $b$ is greater than the budget, and will reject workflow $b$ and
remove it from the schedule. It will then consider workflow $c$, and develop
a low-cost schedule that adds $c$ to the existing schedule for $a$. In order
to complete each job in $c$ by its deadline, SPSS provisions 6 additional
VM-hours, which includes adding an hour to the start time of $VM.2$ for task
$c.60$, adding an hour to the end of $VM.2$ for task $c.45$, adding an hour
to the end of $VM.3$ for $c.50$, adding a new VM, $VM.4$, for 2 hours to run
$c.65$, and finally adding another hour to the end of $VM.2$ for $c.55$. The
resulting schedule has a total cost of 16 VM-hours, or \$16, which is less
than the budget of \$18 and completes both $a$ and $c$ by the deadline.
The exponential score is then $2^{-1}+2^{-3} = 0.625$, which is better than WA-DPDS.

\begin{figure}[htb] 
  \centering
  \includegraphics[width=0.80\columnwidth]{figures/algorithms-example-spss}
  \caption{Schedule produced by SPSS algorithm for the example ensemble from Figure~\ref{fig:example-ensemble}.
  Workflows $a$ and $c$ complete within the given budget and deadline.}
  \label{fig:algorithms-example-spss}
\end{figure}

The example given above illustrates the main concepts of the algorithms using the artificial ensemble.
Two additional example schedules generated using our simulator are shown in 
Figure~\ref{fig:examples-simulated}. 
The schedule generated using DPDS (Figure~\ref{fig:algorithm-example-dpds})
illustrates how tasks from lower priority workflows backfill idle VMs when
tasks from higher priority workflows are not ready for execution.
On the other hand, the schedule generated by SPSS (Figure~\ref{fig:algorithm-example-spss})
shows how SPSS tends to start many workflows in parallel, running each workflow
over a longer period of time on only a few VMs to minimize cost. In comparison,
the dynamic algorithms tend to run one workflow at a time across many VMs in
parallel.

\begin{figure}[tb] 
  \centering
  \subfloat[DPDS]{
  \includegraphics[width=0.80\columnwidth]{figures/output-test-DPDS}
  \label{fig:algorithm-example-dpds}
  }\\
  \subfloat[SPSS]{
  \includegraphics[width=0.80\columnwidth]{figures/output-test-SPSS}
  \label{fig:algorithm-example-spss}
  }
  \caption[Example schedules generated by the algorithms]{Example
  schedules generated by the algorithms. Each row is a different
  VM. Boxes are tasks colored by workflow.}
  \label{fig:examples-simulated}

\end{figure}




\section{Evaluation Methods}
\label{sec:performance}


\subsection{Simulator}

To evaluate and compare the three proposed algorithms, we developed a cloud
workflow simulator~\cite{CloudWorkflowSimulator} based on CloudSim~\cite{Calheiros2011}. Our
simulation model consists of Cloud, VM and WorkflowEngine entities. The Cloud entity starts and
terminates VM entities using an Amazon EC2-like API. VM entities simulate the
execution of individual tasks, including randomized variations in runtime. The
WorkflowEngine entity manages the scheduling of tasks and the provisioning of 
VMs based on the chosen algorithm. We assume that the VMs have a single core
and execute tasks sequentially. 
Although CloudSim provides a more advanced infrastructure model, which includes
time-sharing and space-sharing policies, we do not use these features since we
are interested mainly in the execution of tasks on VMs and high-level workflow
scheduling and provisioning. 
The simulator reads workflow description files in
a modified version of the DAX format used by the Pegasus Workflow Management
System~\cite{Deelman2005}. 
%The modified file format was created for the
%synthetic workflow generator developed by Bharathi, et al~\cite{Bharathi2008}.


\subsection{Workflow Ensembles}
\label{sec:ensembles}


In order to evaluate the algorithms on a standard set of workflows, we created
randomized ensembles using workflows available from the workflow generator
gallery~\cite{WorkflowGenerator}. The
gallery contains synthetic workflows modeled using structures and parameters
that were taken from real applications. Ensembles were created using synthetic
workflows from five real applications: SIPHT, LIGO, Epigenomics, Montage and
CyberShake. For each application, workflows with 50, 100, 200, 300,
400, 500, 600, 700, 800, 900 and 1000 tasks were created. For each workflow
size, 20 different workflow instances were generated using parameters and task
runtime distributions from real workflow traces. The total collection of
synthetic workflows contains 5 applications, 11 different workflow sizes, and 20
workflow instances, for a total of 1100 synthetic workflows.


Using this collection of workflows, we constructed five different ensemble types:
constant, uniform sorted, uniform unsorted, Pareto sorted and Pareto unsorted.
In the unsorted ensembles, workflows of different sizes are mixed together and
the priorities are assigned randomly.
% so that there is no relationship between
%priority and workflow size. 
For many applications, however, large workflows are
more important to users than small workflows because they represent more
significant computations. To model this, the sorted ensembles are sorted by
size, so that the largest workflows have the highest priority.

Constant ensembles contain workflows that all have the same number of tasks. 
%For each constant ensemble, 
The number of tasks is chosen randomly from the set of
possible workflow sizes. Once the size is determined, then N workflows of that
size are chosen randomly for the ensemble from the set of synthetic workflows.
%for the given application.

Uniform ensembles contain workflows with sizes that are uniformly distributed
among the set of possible sizes. Each workflow 
%in a uniform ensemble 
is selected by first randomly choosing the size of the workflow
% according to a uniform distribution, 
and then randomly choosing a workflow of that size from the set of
synthetic workflows.
% for the given application.

Pareto ensembles contain a small number of larger workflows and a large number
of smaller workflows. Their sizes  
%in a Pareto ensemble 
are chosen according to a Pareto distribution. The distribution was modified so
that the number of large workflows (of size $\geq$ 900) is increased by a small amount to
produce a ``heavy-tail''. This causes Pareto ensembles to have a slightly larger
number of large workflows, which reflects behavior commonly observed in many
computational workloads. 
An example of the distribution of workflow sizes that
occurs in a Pareto ensemble is shown in Figure~\ref{fig:ensemble-pareto}.

\begin{figure}[t] 
    \centering
    \includegraphics[width=0.6\columnwidth]{figures/ensemble-pareto}
    \caption[Histogram of workflow sizes in Pareto ensembles]{Histogram of
    workflow sizes in Pareto ensembles. Workflow size is measured in number of
    tasks.}
    \label{fig:ensemble-pareto}
\end{figure}

The number of workflows in an ensemble depends on the particular application,
but we assume that ensemble sizes are on the order of between 10 and 100
workflows, typical of the real applications we have examined (see Section~\ref{sec:intro}). 
For example, the number of
geographical sites of interest to the users of the CyberShake application in
the past has been on the order of 100. 
Second, smaller ensembles consisting of
just a few workflows can be aggregated into a single workflow, so there is no need to
treat them as an ensemble. Similarly, when the number of workflows grows and
each workflow has a large number of tasks, either the deadline and budget
constraints are low enough to prevent many of the workflows from running, or the
problem of efficiently allocating them to the resources becomes similar to a
bag-of-tasks problem, which is easier to solve efficiently.



\subsection{Experimental Parameters}
\label{sec:exp-parameters}

In order to observe the interesting characteristics of the proposed algorithms,
for each ensemble, we selected ranges for deadline and budget that cover a broad
parameter space: from tight constraints, where only a small number of workflows can be completed,
to more liberal constraints where all, or almost all, of the workflows can be
completed. We computed constraint ranges based on
the characteristics of each ensemble. The budget constraints are calculated by
identifying the smallest budget required to execute one of the workflows in the
ensemble (MinBudget), and the smallest budget required to execute all workflows
in the ensemble (MaxBudget):
%
$$
MinBudget = \min_{w\ \in\ e}{Cost(w)}
$$
$$
MaxBudget = \sum_{w\ \in\ e}{Cost(w)}
$$
%
This range---$[MinBudget, MaxBudget]$---is then divided into equal intervals to
determine the budgets to use in each experiment. Similarly, the deadline
constraints are calculated by identifying the smallest amount
of time required to execute a single workflow in the ensemble (MinDeadline),
which is the length of the critical path for the workflow with the shortest
critical path, and by identifying the smallest amount of time required to
execute all workflows (MaxDeadline), which is the sum of the critical paths of
all the workflows:
%
$$
MinDeadline = \min_{w\ \in\ e}{CriticalPath(w)}
$$
$$
MaxDeadline = \sum_{w\ \in\ e}{CriticalPath(w)}
$$
%
This range---$[MinDeadline, MaxDeadline]$---is then divided into equal intervals. 
By computing the budget and deadline constraints in this way we ensure that the
experiments for each ensemble cover the most interesting area of the parameter space for the
ensemble.

In all the experiments we assumed that the VMs have a price of \$1 per VM-hour.
This price was chosen to simplify interpretation of results and should not
affect the relative performance of the different algorithms. In this study the
heterogeneity of the infrastructure is not relevant since we assume that it is
always possible to select a VM type that has the best price to performance ratio
for a given application~\cite{Juve2009}.

All the experiments were run with maximum autoscaling factor ($v_{max}$) set to
1.0 for DPDS and WA-DPDS. After experimenting with DPDS and WA-DPDS we found
that, due to the high parallelism of workflows used, the resource utilization
remains high enough without adjusting the autoscaling rate. Based on experiments
with the target applications, we set the SPSS $\alpha$ parameter for deadline
distribution to be 0.7, which allocates slightly more time to levels with many
tasks.




\section{Discussion of Results}
\label{sec:results}


\subsection{Relative Performance of Algorithms}
\label{sec:relative-performance}


\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{run-finish-variations-test-0-output-distributions}
    \caption{Percentage of high scores achieved by each algorithm on different ensemble types for all five applications. C = Constant ensembles, PS = Pareto Sorted ensembles, PU = Pareto Unsorted ensembles, US = Uniform Sorted ensembles, and UU = Uniform Unsorted ensembles.} 
    \label{fig:distributions}
\end{figure*}




The goal of the first experiment is to characterize the relative performance
of the proposed algorithms. This was done by simulating the algorithms on many
different ensembles and comparing the scores computed using the
performance metric based on exponential scoring defined in
Section~\ref{sec:perf_metric}.

Figure~\ref{fig:distributions} shows the percentage of simulations for which
each algorithm achieved the highest score for a given ensemble type. This
experiment was conducted using all five applications, with all five types of
ensembles. For each application and ensemble type, 10 random ensembles of 50
workflows each were created. Each ensemble was simulated with all three
algorithms using 10 budgets and 10 deadlines (1,000 simulations per
application, ensemble type, and algorithm, or 75,000 in total). The best scores percentage is
computed by counting the number of times that a given algorithm achieved the
highest score and dividing by 1000. Note that it is possible for multiple
algorithms to get the same high score (to tie), so the numbers do not
necessarily add up to 100\%. The sum is much higher than 100\% in cases where
the dynamic algorithms perform relatively well because DPDS and WA-DPDS, which
are very similar algorithms, often get the same high score.



There are several interesting things to notice about
Figure~\ref{fig:distributions}. The first is that, in most cases, SPSS
significantly outperforms both dynamic algorithms (DPDS and WA-DPDS). This is
attributed to the fact that SPSS is able to make more intelligent scheduling
and provisioning decisions because it has the opportunity to compare different
options and choose the one that results in the best outcome. In comparison,
the dynamic algorithms are online algorithms and are not able to project into
the future to weigh the outcomes of their choices.

The second thing to notice is that, for constant ensembles, the dynamic
algorithms perform significantly better relative to SPSS compared to  other
ensemble types. This is a result of the fact that, since all of the workflows
are of approximately the same size and shape, the choice of which workflow to
execute next has a smaller impact on the final result.

We can also see that the workflow-aware
algorithms (WA-DPDS and SPSS) both perform better in most cases than the
simple online algorithm that uses resource utilization alone to make
provisioning decisions (DPDS). This suggests that there is a significant value
in having information about the structure and estimated runtime of a workflow
when making scheduling and provisioning decisions.

Finally, it is interesting that, for Montage and CyberShake, the
relative performance of SPSS is significantly less than it is for other
applications. We attribute this to the structure of Montage and CyberShake.
The workflows for both applications are very wide relative to their height,
and both have very short-running tasks, resulting  in relatively short
critical paths that makes them look more like bag-of-tasks applications, which are easier
to execute than more structured applications. DPDS and WA-DPDS are able to
pack more of the tasks into the available budget and deadline because there
are a) more choices for where to place the tasks, and b) the different choices
have a smaller impact on the algorithms' ability to execute the workflow
within the constraints. In addition, the short critical paths put SPSS at a
disadvantage. Because of the way SPSS assigns deadlines to individual tasks,
it is prevented from starting workflows late, which prevents it from packing
tasks into the idle VM slots at the end of the schedule.

\subsection{Results vs. varying deadline and budget}


\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\textwidth]{run-finish-variations-test-0-output-deadlines-app-dist-selected}    
    \caption{Number of completed workflows from the ensemble depending on normalized deadline.}
    \label{fig:completed-deadlines}
\end{figure*}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.9\textwidth]{run-finish-variations-test-0-output-budgets-app-dist-selected}    
    \caption{Number of completed workflows from the ensemble depending on normalized budget.}
    \label{fig:completed-budgets}
\end{figure*}


In order to give more details about the performance of the algorithms, in this
section we show how the results depend on the deadline and budget constraints.
As the performance metric we use the number of completed workflows of the
ensemble, since the exponential score is not suitable for showing trends.
We use all deadline and budget parameters as defined in
Section~\ref{sec:exp-parameters} and normalize them to the range from 0 to 1 to
facilitate plotting.
For each normalized deadline, we compute the mean number of completed workflows,
averaged over all budgets and random seeds, so each point on the plot is based
on 100 different simulation runs. In Figure~\ref{fig:completed-deadlines}
and~\ref{fig:completed-budgets} we present the results of CyberShake and LIGO
applications, as representatives of fine-grained and coarse-grained workflows.
Similar results were obtained for other applications.

Figure~\ref{fig:completed-deadlines} shows the results depending on normalized
deadline. It can be observed, that with increasing deadline, the number of
completed worklfows grows steeply and then stabilizes. This means that for
longer deadlines it is usually not possible to complete more workflows due to
the budget constraint. For short deadlines, the algorithms need to allocate more
VM instances in parallel, making it more difficult to utilize the resources
efficiently, which results in lower numbers of completed workflows.

We can also observe that the algorithms are able to complete more workflows in
the ensembles that have priorities assigned randomly (unsorted), in comparison
to the ensembes where the priorities are assigned based on workflow size
(sorted). This results from the fact that in the unsorted distributions the large
workflows often have low priorities, so they can be rejected by the algoritms,
thus making more space in the schedule for smaller high-priority workflows.

One interesting observation from Figure~\ref{fig:completed-deadlines} is that for
CyberShake application the result curves have a sawtooth shape. This results
from the combination of two effects. The first comes from the fact that dynamic
algorithms perform better when the deadline is close to the full hour, due to
the provisioning mechanism that tries to shut down unused VM instances before a
full hour. The second one is that when the deadline increases, it may become
possible to admit a larger workflow with higher priority instead of several
smaller ones with lower priority. That is why the number of completed workflows
may decrese, but the exponential score (utility function) based on priorities
will be higher. These effects are visible for CyberShake application, for which
the maximum deadline is 3 hours, but they become less important for LIGO
application, for which the maximum deadline is 20 hours. See also the discussion
on task granularity in Setion~\ref{sec:granularity}.

Fig~\ref{fig:completed-budgets} shows the results based on normalized budget,
each point being the average over all deadline and random seed. What becomes
apparent is that for constant and unsorted distributions the number of completed
workflows is almost linear. This confirms that the budget constraint has the
linear influence on the results, when the influence of the deadline is
eliminated by averaging. On the other hand, for sorted distributions the
functions are convex, increasing slowly for small budgets and steeper for larger
budgets. This reflects the fact that the in sorted ensembles the largest
workflows have the highest priority, so it requires a large increment of a small
budget to fir the next workflow into the schedule.

The chacacteristics discussed in this section can be useful not only for
analysis of our algorithms, but also for planning scientific experiments and
understanding tradeoffs between cost, deadline and the number of work
complete~\cite{Malawski-FGCS-13}.





\subsection{Task Granularity}
\label{sec:granularity}

\begin{figure*}[htb]
    \centering
%    \subfloat[CyberShake]{
    \includegraphics[width=0.6\textwidth]{run-stretching-test-output-scaling-CYBERSHAKE}
%    }\\
%    \subfloat[Montage]{
    \includegraphics[width=0.6\textwidth]{run-stretching-test-output-scaling-MONTAGE}
%    }
    \caption{Percentage of high scores achieved by each algorithm for CyberShake (top)
    and Montage (bottom) ensembles when task runtime is scaled by a constant factor. A
    scaling factor of $x$ means that the runtime of tasks in each workflow was
    multiplied by $x$.}
    \label{fig:stretching}
\end{figure*}



In the experiments described in previous sections we noted that, for Montage and
CyberShake, the short runtimes of their tasks made the dynamic algorithms
perform better relative to SPSS. In order to test this theory we adjusted the
granularity of the tasks in several Montage and CyberShake ensembles to see how
this would affect the relative performance of the algorithms. The granularity
adjustment was achieved by multiplying the runtime of each task by a fixed
scaling factor.

Figure~\ref{fig:stretching} shows the relative performance of the algorithms as
the scaling factor is increased from 1 to 16 for CyberShake  and Montage
applications. Each data point in the figure represents 500 simulations (5
ensembles with 10 budgets and 10 deadlines). The best scores percent was
calculated the same way as it was for Figure~\ref{fig:distributions}.

The figure shows that, as the scaling factor increases beyond 2, the relative
performance of SPSS surpasses that of the dynamic algorithms.  This result
suggests that, in general, for fine-grained workflows the dynamic algorithms
will produce better scores, and for coarse-grained workflows SPSS will produce
better scores.

The granularity of the workflow tasks should be discussed in relation to the
length of the cloud billing cycle. In this paper we assumed that the resources
are billed by the hour, as is the case of VM instances on Amazon EC2.
However, there are cloud providers that have higher billing frequency, e.g.
every 5 minutes in the case of
CloudSigma\footnote{\url{http://www.cloudsigma.com}} or every 1 minute in the
case of Google Compute
Engine\footnote{\url{https://cloud.google.com/pricing/compute-engine}} (after
first 10 minutes). Our results indicate that when the task granularity is close
to the length of the billing period, the SPSS produces better results, while for
tasks that are shorter than the billing period, the dynamic algorithms perform
better. This suggests that these granularity effects have to be taken into account when
selecting a cloud provider, and also when using task clustering algorithms that
group smaller tasks into larger groups~\cite{ChenD12}.



\subsection{Inaccurate Task Runtime Estimates}
\label{sec:variances}

\begin{figure*}[htb]
    \centering
    \subfloat[Ratio of Simulated Cost to Budget]{
        \includegraphics[width=0.38\textwidth]{run-finish-variations-test-0-output-cost-all}
        \label{fig:variances-budget}
    }
    \hspace{2cm}
    \subfloat[Ratio of Simulated Makespan to Deadline]{
        \includegraphics[width=0.38\textwidth]{run-finish-variations-test-0-output-makespan-all}
        \label{fig:variances-deadline}
    }
    \caption{Boxplots for budget and deadline ratios when runtime estimate 
    error varies from $\pm$0\% to $\pm$50\% for all three algorithms. Values 
    greater than 1 indicate that the budget/deadline constraint was exceeded.}
    \label{fig:variances}
\end{figure*}
\begin{figure*}[htb]
    \centering
        \includegraphics[width=0.8\textwidth]{run-finish-variations-test-0-output-distributions-variances}
    \caption{\label{fig:variances-scores}Percentage of high scores achieved by each algorithm on 
    CyberShake and LIGO when runtime estimate error varies from 0\% to 50\%}
\end{figure*}


Both of the workflow-aware algorithms rely on estimates of task runtimes to make
better scheduling and provisioning decisions. Our experience suggests that such
assumption is often reasonable, since we can obtain workflow performance
characteristics from preliminary runs~\cite{Bharathi2008,Deelman2005,Juve2010}.
Some applications, like Periodograms~\cite{Vockler2011} even include a
performance model that estimates task runtimes and automatically annotates
workflow description with these data. In practice, however, these
estimates are often inaccurate. Given inaccuate estimates, the question is: How
do errors in task runtime estimates impact the performance of our scheduling and
provisioning algorithms?  To examine this we introduced uniform errors in the
task runtime and observed the behavior of the algorithms in terms of meeting the
desired budget and deadline constraints.


In this experiment the actual runtime of each task is adjusted in the
simulation by adding a random error to the estimated runtime of $\pm p\%$.
Since the sampling is done uniformly, we expect to get just as many
overestimates as underestimates in any given simulation.

Figure~\ref{fig:variances} shows the results for estimate errors
ranging from $0\%$ to $50\%$. The figure summarizes the outcome of an
extensive suite of 525,000 simulations (10 ensembles of 50 workflows x 5
applications x 5 distributions x 10 budgets x 10 deadlines x 7 error values x
3 algorithms). Box plots show the ratio of the ensemble cost to
budget, and of ensemble makespan to deadline. Whiskers on the plots
indicate maximum and minimum values. The ratio indicates whether the value 
(for example, the simulated cost) exeeded the constraint (the budget). Values
greater than 1 indicate that the constraint was exceeded. 

Figure~\ref{fig:variances-budget} shows the ratio of simulated cost to budget. This
plot illustrates two important algorithm characteristics. First, the dynamic
algorithms very rarely exceeded the budget, even with very large errors. This
indicates that the dynamic algorithms are able to adapt to uncertainties at
runtime to ensure that the constraints are not exceeded, even when the quality
of information available to them is low. Second, unlike the dynamic
algorithms, the static algorithm frequently exceeded the budget constraint; by
large amounts in some cases. This is a result of the fact that the static
algorithm makes all of its decisions ahead of the execution and is not able to adapt
to changing circumstances.

The SPSS plan describes what tasks to execute on which VMs, but not when to
execute them. At runtime, the worklow engine is forced to spend extra money
to extend the runtime of some VMs so that all of the tasks assigned to that
VM can be completed. At the same time, other VMs can be terminated early
because the tasks they were assigned finished earlier than expected. However,
because of dependencies in the workflow, the latter case is less likely to
happen, which causes gaps in the schedule. So the net result is that the
overall cost is increased.

Figure~\ref{fig:variances-deadline} shows the ratio of simulated makespan to budget.
Interestingly, the deadline constraint is rarely exceeded, even in cases with
very poor quality estimates. For the dynamic algorithms this is a result of the
fact that they can adapt to poor estimates and stop submitting new tasks when
the constraints are reached. Makespan is then computed as the finish time of the
last fully completed workflow from the ensemble. For the static algorithm this
is a result of the way that SPSS schedules workflows, and not of a particularly
clever optimization. The SPSS algorithm tends to schedule workflows early, using
up the budget long before the deadline is reached. This is a consequence of the
deadline distribution function in SPSS, which prevents workflows from starting
late. This is illustrated by the Gantt chart in
Figure~\ref{fig:algorithm-example-spss}, which shows how SPSS tends to pile up
workflows at the beginning of the timeline.
In comparison, the dynamic algorithms tend to spread out workflow starts over
the entire duration as shown in Figure~\ref{fig:algorithm-example-dpds}. The
consequence of this behavior is that SPSS plans tend to use up the budget but
leave plenty of time before the deadline.
As a result, when the runtime of the plan is increased by introducing errors,
the SPSS plan has some room to expand without exceeding the deadline.


Figure~\ref{fig:variances-scores} shows relative performance as a percentage of
the number of high scores achieved by each algorithm as the error increases. As
in Figure~\ref{fig:completed-budgets} and ~\ref{fig:completed-deadlines} we show
results for CyberShake and LIGO as representatives of fine-grained and
coarse-grained workflows. The figure shows that, for this experiment, the scores remain
the same as the error increases. This is a result of the way the error is
applied. Since the error is selected uniformly from $-p$ to $+p$ it is equally
likely that the error will be positive as negative. Therefore, applying the
error leaves the total runtime of the tasks in the workflow unchanged -- the
total number of CPU hours in each workflow remains nearly the same regardless of
the error. The end result is that the dynamic algorithms are able to achieve the
same score without exceeding the constraints by using a different schedule that
finishes the same total amount of work using the same budget and deadline.


Note that the algorithms have \textit{not} been changed to account for
inaccurate runtime estimates in this experiment. It is likely that better
performance could be achieved if the algorithms were given a hint as to the
accuracy of the task runtimes. Investigating that optimization is left for
future work. 
The goal here is to determine how well the current algorithms are
able to stay within the constraints given inaccurate task runtime estimates.

It is possible that the static algorithm could be modified to add more
breathing room in the schedule to account for situations where the runtime
estimates may be inaccurate. Such a modification would result in slightly
worse scores when the estimates are good, but would improve scores when the
estimates are bad. Alternatively, the SPSS algorithm could be modified to
generate a plan that specifies when to provision and deprovision each VM. In
that case the workflow engine could be prevented from exceeding the budget
and deadline constraints, but would prevent some workflows from finishing,
which may significantly decrease the score. These two modifications are
subjects for future work.


\subsection{Provisioning Delays}
\label{sec:delays}

One important issue to consider when provisioning resources in the cloud is
the amount of time between when a resource is requested, and when it actually
becomes available to the application. Typically these provisioning delays are
on the order of a few minutes, and are highly dependent upon the cloud architecture
and/or the size of the VM image~\cite{Nurmi2008b}.

\begin{figure*}[htb]
    \centering
    \subfloat[Ratio of Simulated Ensemble Cost to Budget]{
        \includegraphics[width=0.38\textwidth]{run-finish-delays-test-0-output-cost-montage}
        \label{fig:delays-budget}
    }
    \hspace{2cm}
    \subfloat[Ratio of Simulated Ensemble Makespan to Deadline]{    
        \includegraphics[width=0.38\textwidth]{run-finish-delays-test-0-output-dagfinish-montage}
        \label{fig:delays-deadline}
    }
    \caption{Boxplots for budget and deadline ratios when provisioning delay 
    varies from 0 seconds to 15 minutes for all three algorithms. Values 
    greater than 1 indicate that the budget/deadline constraint was exceeded.}
    \label{fig:delays}
\end{figure*}
\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.8\textwidth]{run-finish-delays-test-0-output-distributions-delays}
  \caption{Percentage of high scores achieved by each algorithm on CyberShake and LIGO
  ensembles when provisioning delay varies from 0 seconds to 15 minutes.}
  \label{fig:delays-scores}
\end{figure*}


We assume that resources are billed from the minute that they are requested
until they are terminated.  As a result, provisioning delays have an impact on
both the cost and makespan of an ensemble.

Figure~\ref{fig:delays} shows the ratios of simulated values to constraints when
the provisioning delay is increased from 0 seconds up to 15 minutes. The
figure summarizes a suite of 105,000 simulations (10 ensembles of 50 workflows
x 5 distributions x 10 budgets x 10 deadlines x 7 error values x 3
algorithms). To reduce the simulation time, only one application, Montage, was
used in this experiment. 

As in Figure~\ref{fig:variances}, the y-axis in each
plot represents the ratio of the simulated value to the constraint value for
the budget constraint or the deadline constraint.


The effect of provisioning delays on workflow performance is similar to that of
inaccurate runtime estimates: when the delays are small, all algorithms are able
to produce results within the constraints, but for larger delays, the dynamic
algorithms are able to adapt to avoid exceeding the constraints while the static
algorithm is not. In the case of delays of more than one minute, which is
typical of what has been observed on academic clouds~\cite{Juve2011} such as
Magellan~\cite{Magellan} and FutureGrid~\cite{FutureGrid}, approximately half of
the SPSS simulations exceeded the budget; in some cases by up to a factor of 2.
In comparison, none of the simulations that used a dynamic algorithm exceeded
the budget or the deadline constraint.


Figure~\ref{fig:delays-scores} shows the relative performance as a
percentage of the number of high scores achieved by each algorithm as the
provisioning delay increases. This figure shows that, as the delay
increases, the relative performance of the static algorithm increases as
well. This behavior is a result of the fact that the static algorithm is, in
essence, cheating by using more time and money than the dynamic algorithms.
In the previous section we showed how the relative performance of the
algorithms remains the same when a uniform error is applied to the runtime
constraints. In that case, the dynamic algorithms adapted to the error by
rearranging the schedule to accomplish the same amount of work given the
same deadline and budget. In this case, the dynamic algorithms adapt by
performing less work (executing fewer workflows) to remain within the
constraints while accounting for the delays. In both cases, the static
algorithm completed the same amount of work, but did so by exceeding the
constraints.

This experiment suggests that SPSS is too sensitive to provisioning delays
in its current form to be of practical use in real systems. It is possible
that modifying the SPSS algorithm to account for provisioning delays would
improve its performance on this experiment. In fact, since provisioning
operations are infrequent (because all the algorithms tend to provision
resources for a long time), it is likely that the performance of SPSS could be
improved significantly by simply adding an estimate of the provisioning delay
to its scheduling function. Such an estimate may not have to be particularly
accurate to get good results, and developing an estimate from historical data
should be relatively simple. Testing this idea is left for future work.


\subsection{Task Failures}
\label{sec:failures}

Running workflows consisting of large numbers of tasks on distributed systems
often results in failures. The goal of the next experiment was to assess the
behavior of the algorithms in the presence of task execution failures by
introducing a failure model into the simulator. The model is characterized by
a failure rate $f$ that defines the probability that a task will fail. The
failure time of the task is determined by randomly sampling a value between
the task start time and finish time. If the task fails, it is reported to the
workflow engine, which retries the task until it succeeds. The dynamic
algorithms re-add the task to the priority queue (queue $P$ in
Algorithm~\ref{alg:ds}) so that it can be resubmitted by the scheduler. The
SPSS algorithm immediately  resubmits the failed task to the same VM that was
selected in the plan (to minimize the disruption to the overall plan).

\begin{figure*}[tb]
    \subfloat[Ratio of Simulated Ensemble Cost to Budget]{
        \includegraphics[width=0.38\textwidth]{run-finish-failures-test-0-output-cost-all}
        }
    \hspace{2cm}
    \subfloat[Ratio of Simulated Ensemble Makespan to Deadline]{
        \includegraphics[width=0.38\textwidth]{run-finish-failures-test-0-output-makespan-all}
        }
    \caption{Boxplots for budget and deadline ratios when failure rate varies
    from 0\% to 50\% for all three algorithms. Values greater than 1 indicate 
    that the budget/deadline constraint was exceeded.}
    \label{fig:failures}
\end{figure*}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{run-finish-failures-test-0-output-distributions-failures}
  \caption{Percentage of high scores achieved by each algorithm on CyberShake and LIGO
  ensembles when failure rate varies 0 to 50\%.}
  \label{fig:failures-scores}
\end{figure*}

Figure~\ref{fig:failures} shows the ratios of simulated values to constraints when
the failures are introduced. The figure summarizes a suite of 525,000
simulations (10 ensembles of 50 workflows x 5 applications x 5 distributions x
10 budgets x 10 deadlines x 7 failure rates x 3 algorithms). As in the previous 
experiments, these results show that high failure rates can degrade the
performance of the static algorithm considerably, while the dynamic algorithms
are able to adapt. 

Figure~\ref{fig:failures-scores} shows the relative performance as a percentage of
the number of high scores achieved by each algorithm as the failure rate
increases. The results are similar to the ones of
Figure~\ref{fig:delays-scores}. Comparing Figure~\ref{fig:failures} to
Figs.~\ref{fig:variances} and~\ref{fig:delays} one may conclude that failures
are worse than provisioning delays and runtime estimate errors, since their
impact is larger. However, we consider higher failure rates as rare events that
suggest a significant system malfunction or invalid selection of resources.





\subsection{SPSS Planning Time}

Because SPSS involves more complicated logic than the dynamic algorithms and
makes its decisions before execution, it is important to understand what
impact planning time has on the overall execution time.

Figure~\ref{fig:spss_planning_time} shows the SPSS planning time for ensembles
of 100 workflows with five different workflow sizes: 50, 200, 400, 600, and
800 tasks. The ensembles were generated using a constant distribution equal to
the workflow size desired. Two different applications were used: SIPHT and
CyberShake. Each box summarizes the results of 2000 simulations (2
applications x 10 ensembles x 10 budgets x 10 deadlines).

Figure~\ref{fig:spss_planning_time} shows that, for small workflows, the SPSS
planning time is reasonable, taking on the order of tens of seconds to a few
minutes. For larger ensembles of large workflows, however, the SPSS planning
time can easily reach 10 minutes. Considering that the largest workflows used
in this experiment are still relatively small (maximum of 800 tasks), and that
real workflows are often much larger (workflows with tens of thousands of
tasks are common, and even workflows with millions of
tasks are possible), it is unlikely that SPSS will be practical for
ensembles of very large workflows.


SPSS considers scheduling each task on the cheapest available slot, which
involves scanning all of the available slots on all of the VMs. Since the number
of available slots, in the worst case, is proportional to the number of tasks
scheduled (because scheduling a task splits an existing slot into at most two
slots: one before the task, and one after), the complexity of SPSS is
$O(n^2)$, where $n$ is the number of tasks in the ensemble. In comparison, the
dynamic algorithms all have a more scalable complexity of $O(n)$. DPDS only
examines the tasks in the workflow once when they are scheduled, and WA-DPDS
does it twice: once in the admission algorithm, and once when they are
scheduled. This makes the dynamic algorithms a better fit for larger workflows
and ensembles even though in some cases they may not produce as good results
as SPSS.

It may be possible to optimize SPSS to reduce its runtime by, for example,
clustering the workflow to increase task granularity, which would decrease the
ratio of planning time to ensemble makespan. It may also be possible to reduce
the complexity of SPSS by employing more sophisticated data structures to
store the available slots. Investigating these topics is left for future work.


\begin{figure}[tb]
    \centering
    \vspace{-6mm}
    \includegraphics[width=0.27\textwidth]{spss_planning_time}
    \vspace{-3mm}
    \caption{Planning time of SPSS algorithm for ensembles of 100 workflows and different workflow sizes.}
    \label{fig:spss_planning_time}
\end{figure}

\section{Conclusions and Future Work}
\label{sec:conclusions}

In this paper we addressed the interesting and important new problem of
scheduling and resource provisioning for scientific workflow ensembles on IaaS
clouds. The goal of this work is to maximize the number of user-prioritized
workflows that can be completed given budget and deadline constraints.

This problem differs from previous work on grid and utility grid scheduling
in that cloud infrastructures provide more control over provisioning, so that
the number of resources can be adjusted according to the requirements of the
application. Therefore the problem space becomes larger; it requires not only
an efficient mapping of tasks to available resources, but also the selection
of the best resource provisioning plan.

Formulating the problem as a maximization of the number of prioritized
workflows completed from the ensemble is also novel and requires workflows to
be admitted or rejected based on their estimated resource demands. We believe
that this bi-constrained problem is highly relevant because such constraints
are commonly imposed on many real-world projects. The approach is also
directly applicable to grid environments that provide resource reservations
and charge service units for resource use.

We developed three algorithms to solve this problem: two dynamic algorithms,
DPDS and WA-DPDS, and one static algorithm, SPSS. The algorithms were
evaluated via simulation on ensembles of synthetic workflows, which were
generated based on statistics from real scientific applications.

The results of our simulation studies indicate that the two algorithms that
take into account the structure of the workflow and task runtime estimates
(WA-DPDS and SPSS) yield better results than the simple priority-based
scheduling strategy (DPDS), which makes provisioning decisions based purely on
resource utilization. This underscores the importance of viewing workflow
ensembles as a whole rather than as individual tasks or individual workflows.

In cases where there are no provisioning delays, task runtime estimates are
good, and failures are rare, we found that SPSS performs significantly better
than both dynamic algorithms. However, when conditions are less than perfect,
the static plans produced by SPSS are disrupted and it frequently exceeds the
budget and deadline constraints. In comparison, the dynamic algorithms are able
to adapt to a wide variety of conditions, and rarely exceed the constraints even
with long delays, poor estimates, and high failure rates. However, this comes
at the cost of not being able to complete as many workflows as SPSS in ideal
conditions.

In addition, we found that SPSS tends to perform better on coarse-grained
workflows than the dynamic algorithms. For wide, fine-grained workflows, such
as CyberShake and Montage, however, the dynamic algorithms frequently produce
performance that is as good or better than SPSS, because they are better able
to pack the smaller tasks onto idle VMs close to the deadline than SPSS,
which distributes deadlines to tasks in a way that prevents them from
starting late.

For very large workflows and ensembles, we found that the planning time of
the SPSS algorithm is prohibitive. SPSS often took 10 minutes or more to plan
ensembles of 100 workflows with 800 tasks each. This suggests that ensembles
of workflows with tens of thousands of tasks, which are commonly encountered
in real workflow applications, would take many hours to plan using SPSS. This
makes the dynamic algorithms much more attractive for large scale problems.

This study suggests several areas for future work. Our current approach models
data access as part of the task execution time and does not consider data
storage and transfer costs. In the future we plan to extend the application
and infrastructure model to  include the various data storage options
available on clouds. A previous experimental study \cite{Juve2010} suggests
that the data demands of scientific workflows have a large impact on not only
the execution time, but also on the cost of workflows in commercial clouds. 
In dynamic algorithms, it will be interesting to extend our
utilization-based autoscaling with more advanced workflow-aware strategies
based on feedback control.
We also plan to investigate heterogeneous environments that include multiple VM
types and cloud providers, including private and community clouds, which will
make the problem even more complex and challenging.


The results of this study can be applied to develop tools that assist
researchers in planning their large-scale computational experiments. The
estimates of cost, runtime, and number of workflows completed that can be
obtained from both the static algorithms and from the simulation runs,
constitute valuable hints for planning ensembles and evaluating the
associated trade-offs.


\section*{Acknowledgments}
This work was supported by the NSF under award OCI-0943725 (STCI) and
by the EU FP7 project VPH-Share (269978).

% ACM style
%\bibliographystyle{abbrv}

% IEEE style
%\bibliographystyle{IEEEtran}

% FGCS style
\bibliographystyle{elsarticle-num}

% This balances the columns by inserting a column break at the given reference
%\IEEEtriggeratref{15}

\bibliography{paper}

%\appendix
%\section{Headings in Appendices}

%\balancecolumns

\end{document}
